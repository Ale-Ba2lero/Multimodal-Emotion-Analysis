{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad139ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "\n",
    "from RAVDESS_dataset_util import *\n",
    "from EmoClassCNN import *\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93157fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/home/studenti/ballerini/datasets/RAVDESS_frames'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b95168e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size:  720\n",
      "test set size:  6480\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = len(emocat)\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 8\n",
    "DEFAULT_Z_DIM = 50\n",
    "\n",
    "face_dataset = FaceEmotionDataset(root_dir=folder_path,\n",
    "                                    transform=transforms.Compose([\n",
    "                                        Rescale(IMG_SIZE), \n",
    "                                        CenterCrop(IMG_SIZE), \n",
    "                                        ToTensor()\n",
    "                                    ]))        \n",
    "\n",
    "trainingset_len = len(face_dataset) // 100 * 10\n",
    "testset_len = len(face_dataset) - trainingset_len\n",
    "\n",
    "print('training set size: ', trainingset_len)\n",
    "print('test set size: ', testset_len)\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(face_dataset, \n",
    "                                                    [trainingset_len, testset_len], \n",
    "                                                    generator=torch.Generator().manual_seed(42)\n",
    "                                                   )\n",
    "\n",
    "trainset_loader = DataLoader(train_set, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, num_workers=4)\n",
    "\n",
    "testset_loader = DataLoader(test_set, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, num_workers=4)\n",
    "\n",
    "dataset_loader = (trainset_loader, testset_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e785e94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_rating_conversion(cat):\n",
    "    ratings = torch.zeros(NUM_CLASSES)\n",
    "    ratings[cat] = 1\n",
    "    return ratings\n",
    "    \n",
    "#torch.argmax(emotion_rating_conversion(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53b28f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "class Swish(nn.Module):\n",
    "    \"\"\"https://arxiv.org/abs/1710.05941\"\"\"\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e70d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductOfExperts(nn.Module):\n",
    "    \"\"\"\n",
    "    Return parameters for product of independent experts.\n",
    "    See https://arxiv.org/pdf/1410.7827.pdf for equations.\n",
    "\n",
    "    @param loc: M x D for M experts\n",
    "    @param scale: M x D for M experts\n",
    "    \"\"\"\n",
    "    def forward(self, loc, scale, eps=1e-8):\n",
    "        scale = scale + eps # numerical constant for stability\n",
    "        # precision of i-th Gaussian expert (T = 1/sigma^2)\n",
    "        T = 1. / scale\n",
    "        product_loc = torch.sum(loc * T, dim=0) / torch.sum(T, dim=0)\n",
    "        product_scale = 1. / torch.sum(T, dim=0)\n",
    "        return product_loc, product_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83b3c741",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    define the PyTorch module that parametrizes q(z|image).\n",
    "    This goes from images to the latent z\n",
    "    \n",
    "    This is the standard DCGAN architecture.\n",
    "\n",
    "    @param z_dim: integer\n",
    "                  size of the tensor representing the latent random variable z\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, img_size):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, \n",
    "        #                padding=0, dilation=1, groups=1, bias=True)\n",
    "        # H_out = floor( (H_in + 2*padding - dilation(kernel_size-1) -1) / stride    +1)\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, 1, 1, bias=False),\n",
    "            Swish(),\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3, 1, 1, bias=False),\n",
    "            Swish(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, 1, 1, bias=False),\n",
    "            Swish(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, 1, 1, bias=False),\n",
    "            Swish(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        \n",
    "        # Here, we define two layers, one to give z_loc and one to give z_scale\n",
    "        self.z_loc_layer = nn.Sequential(\n",
    "            nn.Linear(256 * (self.img_size // 4)**2, 512),\n",
    "            Swish(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, z_dim))\n",
    "        \n",
    "        self.z_scale_layer = nn.Sequential(\n",
    "            nn.Linear(256 * (self.img_size // 4)**2, 512),\n",
    "            Swish(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, z_dim))\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    def forward(self, image):\n",
    "        hidden = self.features(image)\n",
    "        hidden = hidden.view(-1, 256 * (self.img_size // 4)**2)\n",
    "        z_loc = self.z_loc_layer(hidden)\n",
    "        z_scale = torch.exp(self.z_scale_layer(hidden)) #add exp so it's always positive\n",
    "        return z_loc, z_scale\n",
    "    \n",
    "class ImageDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    define the PyTorch module that parametrizes p(image|z).\n",
    "    This goes from the latent z to the images\n",
    "    \n",
    "    This is the standard DCGAN architecture.\n",
    "\n",
    "    @param z_dim: integer\n",
    "                  size of the tensor representing the latent random variable z\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, img_size):\n",
    "        super(ImageDecoder, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256 * (self.img_size**2)),\n",
    "            Swish())\n",
    "        \n",
    "        self.hallucinate = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            Swish(),\n",
    "            nn.ConvTranspose2d(128, 64, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            Swish(),\n",
    "            nn.ConvTranspose2d(64, 32, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            Swish(),\n",
    "            nn.ConvTranspose2d(32, 3, 3, 1, 1, bias=False))\n",
    "\n",
    "    def forward(self, z):\n",
    "        # the input will be a vector of size |z_dim|\n",
    "        z = self.upsample(z)\n",
    "        z = z.view(-1, 256, self.img_size, self.img_size)\n",
    "        \n",
    "        # but if 100x100, the output image size is 96x96\n",
    "        image = self.hallucinate(z) # this is the image\n",
    "        return image  # NOTE: no sigmoid here. See train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66c2eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    define the PyTorch module that parametrizes q(z|emotion category).\n",
    "    This goes from ratings to the latent z\n",
    "\n",
    "    @param z_dim: integer\n",
    "                  size of the tensor representing the latent random variable z\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim):\n",
    "        super(EmotionEncoder, self).__init__()\n",
    "        self.net = nn.Linear(NUM_CLASSES, 512)\n",
    "        \n",
    "        self.z_loc_layer = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            Swish(),\n",
    "            nn.Linear(512, z_dim))\n",
    "        \n",
    "        self.z_scale_layer = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            Swish(),\n",
    "            nn.Linear(512, z_dim))\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    def forward(self, emocat):\n",
    "        hidden = self.net(emocat)\n",
    "        z_loc = self.z_loc_layer(hidden)\n",
    "        z_scale = torch.exp(self.z_scale_layer(hidden))\n",
    "        return z_loc, z_scale\n",
    "\n",
    "\n",
    "class EmotionDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    define the PyTorch module that parametrizes p(emotion category|z).\n",
    "    This goes from the latent z to the ratings\n",
    "\n",
    "    @param z_dim: integer\n",
    "                  size of the tensor representing the latent random variable z\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim):\n",
    "        super(EmotionDecoder, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(z_dim, 512),\n",
    "            Swish())\n",
    "        \n",
    "        self.emotion_loc_layer = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            Swish(),\n",
    "            nn.Linear(512, len(emocat)))\n",
    "        \n",
    "        self.emotion_scale_layer = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            Swish(),\n",
    "            nn.Linear(512, NUM_CLASSES))\n",
    "\n",
    "    def forward(self, z):\n",
    "        #batch_size = z.size(0)\n",
    "        hidden = self.net(z)\n",
    "        emotion_loc = self.emotion_loc_layer(hidden)\n",
    "        emotion_scale = torch.exp(self.emotion_scale_layer(hidden))\n",
    "        # rating is going to be a |emotions| * 9 levels\n",
    "        #rating = h.view(batch_size, EMOTION_VAR_DIM, 9)\n",
    "        return emotion_loc, emotion_scale  # NOTE: no softmax here. See train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dca7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    This class encapsulates the parameters (neural networks), models & guides needed to train a\n",
    "    multimodal variational auto-encoder.\n",
    "    Modified from https://github.com/mhw32/multimodal-vae-public\n",
    "    Multimodal Variational Autoencoder.\n",
    "\n",
    "    @param z_dim: integer\n",
    "                  size of the tensor representing the latent random variable z\n",
    "                  \n",
    "    Currently all the neural network dimensions are hard-coded; \n",
    "    in a future version will make them be inputs into the constructor\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, img_size=128, use_cuda=True):\n",
    "        super(MVAE, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.img_size = img_size\n",
    "        self.experts = ProductOfExperts()\n",
    "        self.image_encoder = ImageEncoder(z_dim, img_size)\n",
    "        self.image_decoder = ImageDecoder(z_dim, img_size)\n",
    "        self.emotion_encoder = EmotionEncoder(z_dim)\n",
    "        self.emotion_decoder =EmotionDecoder(z_dim)\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        # relative weights of losses in the different modalities\n",
    "        self.LAMBDA_IMAGES = 1.0\n",
    "        self.LAMBDA_RATINGS = 50.0\n",
    "        self.LAMBDA_OUTCOMES = 100.0\n",
    "        \n",
    "        # using GPUs for faster training of the networks\n",
    "        if self.use_cuda:\n",
    "            self.cuda()\n",
    "            \n",
    "    def model(self, images=None, emotions=None, annealing_beta=1.0):\n",
    "        # register this pytorch module and all of its sub-modules with pyro\n",
    "        pyro.module(\"mvae\", self)\n",
    "        \n",
    "        batch_size = 0\n",
    "        if images is not None:\n",
    "            batch_size = images.size(0)\n",
    "        elif emotions is not None:\n",
    "            batch_size = emotions.size(0)\n",
    "        \n",
    "        with pyro.plate(\"data\"):      \n",
    "            \n",
    "            # sample the latent z from the (constant) prior, z ~ Normal(0,I)\n",
    "            z_prior_loc  = torch.zeros(size=[BATCH_SIZE, self.z_dim])\n",
    "            z_prior_scale = torch.exp(torch.zeros(size=[BATCH_SIZE, self.z_dim]))                \n",
    "            \n",
    "            # sample from prior (value will be sampled by guide when computing the ELBO)\n",
    "            with poutine.scale(scale=annealing_beta):\n",
    "                z = pyro.sample(\"z\", dist.Normal(z_prior_loc, z_prior_scale))\n",
    "\n",
    "            # decode the latent code z (image decoder)\n",
    "            img_loc = self.image_decoder.forward(z)\n",
    "            \n",
    "            # score against actual images\n",
    "            if images is not None:\n",
    "                with poutine.scale(scale=self.LAMBDA_IMAGES):\n",
    "                    print('image loc: ', img_loc.shape)\n",
    "                    print('image shape: ', images.shape)\n",
    "                    pyro.sample(\"obs_img\", dist.Bernoulli(img_loc), obs=images)\n",
    "            \n",
    "            # decode the latent code z (emotion decoder)\n",
    "            emotion_loc, emotion_scale = self.emotion_decoder.forward(z)\n",
    "            if emotions is not None:\n",
    "                with poutine.scale(scale=self.LAMBDA_RATINGS):\n",
    "                    pyro.sample(\"obs_emotion\", \n",
    "                                dist.Normal(emotion_loc, emotion_scale), \n",
    "                                obs=emotions)\n",
    "\n",
    "            # return the loc so we can visualize it later\n",
    "            return img_loc, emotion_loc\n",
    "        \n",
    "    def guide(self, images=None, emotions=None, annealing_beta=1.0):\n",
    "        # register this pytorch module and all of its sub-modules with pyro\n",
    "        pyro.module(\"mvae\", self)\n",
    "        \n",
    "        batch_size = 0\n",
    "        if images is not None:\n",
    "            batch_size = images.size(0)\n",
    "        elif emotions is not None:\n",
    "            batch_size = emotions.size(0)\n",
    "            \n",
    "        with pyro.plate(\"data\"):\n",
    "            # use the encoder to get the parameters used to define q(z|x)\n",
    "                        \n",
    "            # initialize the prior expert.\n",
    "            # we initalize an additional dimension, along which we concatenate all the \n",
    "            #   different experts.\n",
    "            # self.experts() then combines the information from these different modalities\n",
    "            #   by multiplying the gaussians together\n",
    "            \n",
    "            z_loc = torch.zeros(torch.Size((1, batch_size, self.z_dim))) + 0.5\n",
    "            z_scale = torch.ones(torch.Size((1, batch_size, self.z_dim))) * 0.1\n",
    "            if self.use_cuda:\n",
    "                z_loc, z_scale = z_loc.cuda(), z_scale.cuda()\n",
    "                \n",
    "            if images is not None:\n",
    "                image_z_loc, image_z_scale = self.image_encoder.forward(images)\n",
    "                z_loc = torch.cat((z_loc, image_z_loc.unsqueeze(0)), dim=0)\n",
    "                z_scale = torch.cat((z_scale, image_z_scale.unsqueeze(0)), dim=0)\n",
    "            \n",
    "            if emotions is not None:\n",
    "                emotion_z_loc, emotion_z_scale = self.emotion_encoder.forward(emotions)\n",
    "                z_loc = torch.cat((z_loc, emotion_z_loc.unsqueeze(0)), dim=0)\n",
    "                z_scale = torch.cat((z_scale, emotion_z_scale.unsqueeze(0)), dim=0)\n",
    "            \n",
    "            z_loc, z_scale = self.experts(z_loc, z_scale)\n",
    "            # sample the latent z\n",
    "            with poutine.scale(scale=annealing_beta):\n",
    "                pyro.sample(\"latent\", dist.Normal(z_loc, z_scale))\n",
    "                \n",
    "                \n",
    "    def forward(self, image=None, emotion=None):\n",
    "        z_loc, z_scale  = self.infer(image, emotion)\n",
    "        z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).independent(1))\n",
    "        # reconstruct inputs based on that gaussian\n",
    "        image_recon = self.image_decoder(z)\n",
    "        rating_recon = self.emotion_decoder(z)\n",
    "        return image_recon, rating_recon, z_loc, z_scale\n",
    "    \n",
    "    \n",
    "    def infer(self, images=None, emotions=None):\n",
    "        batch_size = 0\n",
    "        if images is not None:\n",
    "            batch_size = images.size(0)\n",
    "        elif emotions is not None:\n",
    "            batch_size = emotions.size(0)\n",
    "            \n",
    "        # initialize the prior expert\n",
    "        # we initalize an additional dimension, along which we concatenate all the \n",
    "        #   different experts.\n",
    "        # self.experts() then combines the information from these different modalities\n",
    "        #   by multiplying the gaussians together\n",
    "        z_loc = torch.zeros(torch.Size((1, BATCH_SIZE, self.z_dim))) + 0.5\n",
    "        z_scale = torch.ones(torch.Size((1, BATCH_SIXE, self.z_dim))) * 0.1\n",
    "        if self.use_cuda:\n",
    "            z_loc, z_scale = z_loc.cuda(), z_scale.cuda()\n",
    "\n",
    "        if images is not None:\n",
    "            image_z_loc, image_z_scale = self.image_encoder.forward(images)\n",
    "            z_loc = torch.cat((z_loc, image_z_loc.unsqueeze(0)), dim=0)\n",
    "            z_scale = torch.cat((z_scale, image_z_scale.unsqueeze(0)), dim=0)\n",
    "\n",
    "        if emotions is not None:\n",
    "            emotion_z_loc, emotion_z_scale = self.emotion_encoder.forward(emotions)\n",
    "            z_loc = torch.cat((z_loc, emotion_z_loc.unsqueeze(0)), dim=0)\n",
    "            z_scale = torch.cat((z_scale, emotion_z_scale.unsqueeze(0)), dim=0)\n",
    "\n",
    "        z_loc, z_scale = self.experts(z_loc, z_scale)\n",
    "        return z_loc, z_scale\n",
    "\n",
    "    \n",
    "    # define a helper function for reconstructing images\n",
    "    def reconstruct_img(self, images):\n",
    "        # encode image x\n",
    "        z_loc, z_scale = self.image_encoder(images)\n",
    "        # sample in latent space\n",
    "        z = dist.Normal(z_loc, z_scale).sample()\n",
    "        # decode the image (note we don't sample in image space)\n",
    "        img_loc = self.image_decoder.forward(z)\n",
    "        return img_loc\n",
    "\n",
    "    \n",
    "    # define a helper function for reconstructing images without sampling\n",
    "    def reconstruct_img_nosample(self, images):\n",
    "        # encode image x\n",
    "        z_loc, z_scale = self.image_encoder(images)\n",
    "        ## sample in latent space\n",
    "        #z = dist.Normal(z_loc, z_scale).sample()\n",
    "        # decode the image (note we don't sample in image space)\n",
    "        img_loc = self.image_decoder.forward(z_loc)\n",
    "        return img_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43fc48dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "class Args:\n",
    "    learning_rate = 5e-6\n",
    "    num_epochs = 2 #500\n",
    "    z_dim = DEFAULT_Z_DIM\n",
    "    img_size = IMG_SIZE\n",
    "    seed = 30\n",
    "    cuda = False\n",
    "    \n",
    "args = Args()\n",
    "\n",
    "# setup the VAE\n",
    "mvae = MVAE(z_dim=args.z_dim, img_size=args.img_size, use_cuda=args.cuda)\n",
    "\n",
    "# setup the optimizer\n",
    "adam_args = {\"lr\": args.learning_rate}\n",
    "optimizer = Adam(adam_args)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(mvae.model, mvae.guide, optimizer, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d54b122a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"sample = next(iter(testset_loader))\\nimages = sample['image']\\nemotions = torch.stack([emotion_rating_conversion(emo) for emo in sample['cat']])\\npyro.render_model(mvae.model, model_args=(images, emotions))\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''sample = next(iter(testset_loader))\n",
    "images = sample['image']\n",
    "emotions = torch.stack([emotion_rating_conversion(emo) for emo in sample['cat']])\n",
    "pyro.render_model(mvae.model, model_args=(images, emotions))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca984027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:08, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image loc:  torch.Size([8, 3, 128, 128])\n",
      "image shape:  torch.Size([8, 3, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter probs (Tensor of shape (8, 3, 128, 128)) of distribution Bernoulli(probs: torch.Size([8, 3, 128, 128])) to satisfy the constraint Interval(lower_bound=0.0, upper_bound=1.0), but found invalid values:\ntensor([[[[-3.9017e-02, -7.2188e-01,  2.9513e-01,  ...,  7.0698e-01,\n           -4.0400e-01,  1.1426e+00],\n          [ 9.4293e-01, -4.2566e-01, -1.4508e+00,  ...,  4.9947e-01,\n           -2.5893e-02,  2.5175e-01],\n          [-2.8492e-02,  8.6708e-01, -8.8784e-01,  ..., -3.9668e-01,\n           -3.1076e-01, -7.0688e-01],\n          ...,\n          [ 1.3940e+00, -3.6874e-01, -4.3644e-01,  ...,  5.4321e-01,\n            1.7384e-01, -2.0410e-01],\n          [-1.0094e+00,  5.3915e-01, -5.9737e-02,  ...,  1.2016e+00,\n            3.0995e-01, -2.8569e-01],\n          [ 4.6490e-01, -7.2065e-01, -1.7153e+00,  ...,  7.9064e-02,\n            3.3914e-01,  2.3666e-01]],\n\n         [[-2.7222e-01,  2.8789e-01,  2.0775e+00,  ...,  2.8436e-01,\n            3.6272e-01,  2.2622e-01],\n          [ 8.0380e-01, -1.9356e-01,  1.0928e+00,  ...,  2.8926e-01,\n            4.8483e-01, -5.4913e-02],\n          [-4.5173e-01,  1.3452e-01,  1.2322e+00,  ...,  2.3791e+00,\n           -8.5602e-02,  8.2655e-02],\n          ...,\n          [ 8.3299e-01, -2.5927e-01, -1.6366e+00,  ..., -8.3680e-02,\n           -4.8813e-01,  1.5064e+00],\n          [ 4.6731e-01, -2.6187e-01, -4.0458e-01,  ...,  1.1724e-01,\n           -1.0972e+00,  1.8863e-02],\n          [ 4.0596e-01, -2.4951e-01, -1.5650e+00,  ..., -4.2743e-01,\n           -5.7526e-02, -9.2211e-01]],\n\n         [[ 9.8827e-02,  3.6070e-01, -7.6037e-01,  ..., -7.8843e-01,\n            1.2897e+00,  1.6489e-01],\n          [ 3.5030e-01,  6.3007e-01,  3.2396e-01,  ...,  1.2788e-01,\n            6.0136e-02,  7.2336e-01],\n          [-4.6332e-01,  4.1395e-01, -4.8604e-01,  ..., -5.4677e-02,\n           -1.0098e+00,  4.1479e-01],\n          ...,\n          [-4.2682e-01,  8.7209e-01,  3.5469e-01,  ..., -5.6740e-01,\n           -5.8024e-01, -1.1075e+00],\n          [ 7.4761e-02,  1.1156e+00,  6.0063e-01,  ..., -4.3437e-01,\n           -5.6222e-01,  7.8487e-02],\n          [-9.6210e-02,  2.1367e+00,  2.4944e-01,  ...,  5.7961e-01,\n           -8.0608e-01,  4.8292e-01]]],\n\n\n        [[[ 2.5746e-02, -6.8964e-01,  3.0840e-01,  ...,  1.6439e-01,\n            5.7820e-01,  8.0057e-01],\n          [-1.6396e-01, -4.4783e-01,  7.2353e-01,  ...,  1.2401e-01,\n            1.4810e+00, -3.7449e-01],\n          [-3.9761e-01,  9.5291e-01, -5.1215e-02,  ..., -6.3679e-01,\n            1.1281e+00,  8.2581e-01],\n          ...,\n          [-1.9110e-02,  7.2256e-01,  3.0355e-01,  ..., -3.8227e-01,\n           -3.8176e-01, -5.8665e-01],\n          [ 2.3904e-01,  3.2932e-01,  2.6372e-01,  ...,  5.6213e-01,\n            8.7643e-01,  3.2533e-01],\n          [ 7.6803e-01,  6.2389e-01, -3.6977e-01,  ..., -3.1811e-03,\n            6.8925e-01, -1.6380e-01]],\n\n         [[ 4.5422e-01, -1.2126e-01,  1.7723e-03,  ...,  1.4019e-01,\n           -2.3005e-01,  8.7588e-02],\n          [-1.1691e-01,  6.0980e-01,  6.2077e-01,  ...,  3.9957e-02,\n            5.8375e-01,  5.4512e-01],\n          [ 9.6048e-02,  3.3861e-01, -1.2552e-01,  ..., -8.0926e-01,\n            1.1439e+00,  8.8580e-01],\n          ...,\n          [ 4.9670e-01, -5.3180e-01,  1.0598e+00,  ..., -4.0450e-01,\n            9.5854e-01,  2.3169e-01],\n          [-4.2756e-01,  1.3666e-01, -3.2273e-01,  ...,  4.3074e-01,\n            5.4341e-01, -1.0114e-01],\n          [ 5.4346e-02,  1.0331e+00, -1.1682e-01,  ..., -6.9097e-01,\n           -2.8787e-02, -2.3072e-02]],\n\n         [[-2.0442e-01,  2.5657e-01, -3.3531e-01,  ...,  1.2308e-01,\n           -6.5697e-01,  2.2010e-02],\n          [-8.1277e-01, -1.3558e+00, -2.1419e-01,  ..., -9.4393e-01,\n           -8.5543e-01, -9.2960e-01],\n          [ 4.6743e-01,  2.6072e-01,  2.9905e-01,  ..., -1.7642e-02,\n           -2.4371e+00,  2.9656e-02],\n          ...,\n          [-6.4899e-01,  1.1591e+00,  1.5636e-01,  ..., -1.1162e+00,\n           -6.4924e-01,  9.2095e-02],\n          [ 3.9833e-01,  3.8702e-01, -1.1339e-01,  ..., -1.6524e-01,\n           -1.2188e+00,  7.1567e-01],\n          [-1.3767e-01, -1.6588e-01, -4.0546e-01,  ...,  1.0771e-01,\n            7.1223e-01, -4.5778e-01]]],\n\n\n        [[[ 9.9965e-01, -3.0301e-01,  8.8343e-02,  ..., -3.1927e-01,\n            5.9005e-01,  1.8413e-01],\n          [ 6.5223e-01, -4.7682e-01, -1.1774e+00,  ..., -3.5193e-02,\n           -1.1047e+00, -1.2156e+00],\n          [-9.2792e-01,  1.3876e-01,  1.2122e-01,  ...,  8.2273e-01,\n           -3.3880e-01,  1.3593e-01],\n          ...,\n          [ 8.6760e-02,  1.7300e+00, -4.2076e-01,  ..., -2.6218e+00,\n            1.0819e-01,  6.8241e-01],\n          [ 1.0043e+00,  8.6387e-01,  1.7983e+00,  ..., -8.7246e-01,\n            2.0311e+00,  1.0811e+00],\n          [ 6.1507e-01, -2.8476e-02, -1.0656e+00,  ..., -5.1055e-01,\n           -7.3141e-01, -5.9831e-01]],\n\n         [[ 4.7994e-01, -7.9971e-02,  3.6339e-01,  ...,  1.3162e-02,\n            1.8769e-01,  3.3493e-02],\n          [-5.1917e-01,  4.4698e-01, -6.3489e-01,  ...,  5.7594e-01,\n            2.7531e-01,  7.4175e-01],\n          [-1.5960e-01, -1.2524e+00, -5.0536e-01,  ...,  7.3972e-01,\n            1.3050e+00,  1.4190e+00],\n          ...,\n          [ 5.7906e-01, -3.1778e-01, -7.4224e-01,  ...,  7.5666e-01,\n           -4.8276e-01,  1.7860e+00],\n          [ 9.1892e-03,  2.0764e-01,  8.6349e-03,  ..., -1.0164e-01,\n            8.2697e-01,  3.9829e-01],\n          [ 1.7038e-01,  4.6167e-01, -2.0694e-01,  ..., -7.7880e-01,\n            5.9491e-01,  3.4319e-01]],\n\n         [[ 1.0492e-01, -1.8227e-01, -1.8404e-02,  ...,  6.7217e-01,\n           -7.3851e-01, -2.3027e-01],\n          [-1.1986e-01, -1.8996e-01,  1.9871e-01,  ...,  8.3738e-01,\n           -9.8994e-01, -6.1400e-01],\n          [-1.0485e+00,  8.9330e-01,  2.2865e-01,  ...,  3.4802e-01,\n           -8.6168e-01,  1.1340e+00],\n          ...,\n          [-2.3495e-01,  1.9848e-01,  8.0362e-01,  ...,  1.1575e+00,\n           -1.2087e+00, -2.8792e-01],\n          [ 9.2640e-01,  1.1055e+00,  2.6067e-01,  ..., -1.1524e+00,\n            1.1371e+00, -8.4982e-01],\n          [ 6.1988e-01,  3.4533e-02,  3.1822e-01,  ...,  7.8782e-01,\n           -6.0970e-01, -4.8516e-01]]],\n\n\n        ...,\n\n\n        [[[-1.4784e-01,  2.7171e-01, -3.8212e-01,  ...,  1.0399e-01,\n           -2.5261e-01,  9.2444e-02],\n          [ 1.7680e-01, -6.6754e-01, -7.7115e-02,  ...,  2.1930e+00,\n            2.3621e-01,  1.1916e+00],\n          [-2.6279e-02, -4.1784e-01, -8.3798e-01,  ...,  8.0678e-02,\n            7.1473e-01,  2.9554e-01],\n          ...,\n          [ 9.0389e-01,  1.9255e-01, -1.5749e-01,  ..., -5.9989e-01,\n            1.2586e+00, -6.7517e-01],\n          [ 2.7228e-01,  1.6775e+00,  7.5060e-01,  ...,  1.6830e+00,\n           -3.8423e-01,  1.4338e-01],\n          [ 2.4646e-01, -3.8633e-01, -7.3344e-02,  ...,  1.1645e+00,\n            4.7968e-01, -1.2715e-01]],\n\n         [[ 4.3851e-02,  2.6447e-01,  1.1777e-01,  ..., -3.8761e-02,\n           -2.0988e-01, -4.7920e-02],\n          [ 3.9321e-01,  2.6125e-01,  1.8839e-01,  ...,  5.7798e-01,\n            3.9192e-01,  6.7977e-01],\n          [ 4.0889e-01,  3.9318e-01,  2.7429e-01,  ..., -1.9893e-01,\n            1.2501e+00, -4.4598e-01],\n          ...,\n          [-7.0468e-01,  1.1383e+00, -9.6469e-01,  ...,  1.2142e+00,\n           -5.5559e-01,  6.0136e-01],\n          [-4.2415e-01,  7.2701e-01,  2.7348e-01,  ...,  9.7252e-01,\n            5.3545e-02,  8.9845e-01],\n          [-3.5599e-02, -9.9147e-01,  1.0962e-01,  ..., -4.2500e-01,\n            5.9879e-03,  6.6595e-02]],\n\n         [[ 1.3414e-01,  1.5417e-01,  8.2356e-01,  ..., -6.4451e-01,\n           -8.2499e-01, -1.1627e-01],\n          [ 4.8968e-01,  6.5762e-01,  1.0189e-01,  ..., -5.5968e-01,\n           -1.2620e+00, -5.7268e-01],\n          [-5.3173e-01, -4.4683e-02,  1.6379e-01,  ..., -1.4492e+00,\n           -3.4923e-01,  1.3164e-01],\n          ...,\n          [-3.4171e-01, -6.5994e-01, -2.3982e-01,  ...,  8.3372e-01,\n           -1.5697e-02, -1.0999e+00],\n          [-8.5438e-01,  7.9687e-01,  1.7074e-01,  ...,  4.1932e-01,\n            4.3002e-01, -5.2091e-01],\n          [ 1.2794e-01,  7.3050e-01,  4.6818e-01,  ..., -2.1569e-01,\n           -1.1805e-01, -6.0279e-02]]],\n\n\n        [[[-2.6010e-01,  3.3723e-01, -2.1189e-01,  ...,  5.4050e-01,\n            1.1406e+00, -2.3828e-02],\n          [ 3.8747e-01, -3.9276e-01, -6.5514e-01,  ...,  4.9900e-02,\n           -2.9840e-02,  1.8531e-02],\n          [ 7.1674e-01,  2.7109e-01, -1.4508e+00,  ..., -2.6001e+00,\n           -1.2457e-01, -3.4741e-01],\n          ...,\n          [ 6.7617e-01,  7.4334e-01,  8.9434e-01,  ...,  9.9241e-01,\n            5.7569e-01,  3.0964e-01],\n          [ 3.0716e-01,  9.5590e-01,  1.5069e+00,  ...,  1.3535e+00,\n           -5.4177e-01, -3.1954e-01],\n          [-5.0877e-01, -1.0374e+00,  1.2553e-01,  ..., -3.2210e-01,\n            5.3504e-01, -4.4945e-01]],\n\n         [[-6.6355e-01,  1.5975e-02, -1.1028e+00,  ...,  4.2119e-01,\n            1.4300e-01, -1.0500e+00],\n          [ 2.9872e-02,  8.8631e-01, -6.1298e-01,  ...,  7.4856e-01,\n           -2.5026e-01, -6.9316e-01],\n          [-3.7613e-01, -1.6566e+00, -4.7379e-01,  ...,  1.3844e+00,\n           -2.1917e-01, -3.4775e-02],\n          ...,\n          [ 6.4191e-03, -6.4329e-01, -3.4590e-01,  ...,  7.1099e-01,\n           -8.6027e-01,  1.9707e+00],\n          [ 4.9977e-01,  8.7028e-01, -1.2418e+00,  ...,  4.7148e-02,\n           -2.1899e+00,  5.0688e-02],\n          [-4.3150e-01, -9.5305e-01,  6.0477e-03,  ...,  4.6361e-02,\n            1.2867e+00, -5.2287e-01]],\n\n         [[ 2.1083e-01, -3.4744e-01, -8.8930e-01,  ..., -1.5877e+00,\n            7.6081e-01,  8.1642e-01],\n          [-7.2559e-01,  1.0759e-03,  5.0080e-01,  ..., -9.0819e-01,\n           -1.3664e+00, -5.0009e-01],\n          [ 9.8828e-01,  2.0458e-01, -3.2092e-01,  ..., -1.0505e+00,\n            7.6977e-01, -1.0954e+00],\n          ...,\n          [ 3.1315e-01, -1.8013e-01,  6.6712e-03,  ..., -3.4387e-01,\n            6.7799e-02, -7.9891e-01],\n          [ 9.3795e-01,  8.4810e-01, -3.3065e-02,  ..., -1.4951e+00,\n            1.8487e-01, -7.2444e-01],\n          [-2.9354e-01,  4.9934e-01,  1.5927e+00,  ...,  6.8559e-01,\n           -5.8828e-02,  7.8406e-02]]],\n\n\n        [[[-2.7788e-01, -5.9287e-01, -1.0850e+00,  ...,  1.1521e-01,\n           -1.0350e-01, -1.0359e-01],\n          [ 9.7773e-02, -6.6024e-01,  1.3205e+00,  ...,  5.3065e-01,\n            1.4022e+00, -1.3554e+00],\n          [ 1.2476e-01, -3.5420e-01, -7.5356e-01,  ..., -1.4845e+00,\n           -8.2950e-01, -3.1953e-01],\n          ...,\n          [-2.2498e-01,  1.1432e+00, -6.3102e-02,  ...,  2.2542e+00,\n           -4.3919e-02, -4.5381e-01],\n          [-1.1767e-01,  3.8476e-01,  2.8890e-01,  ...,  5.0568e-01,\n            1.9877e+00, -4.0198e-01],\n          [ 8.3461e-01, -3.8473e-01, -6.2184e-01,  ...,  1.2079e-01,\n            1.2926e+00, -9.2583e-01]],\n\n         [[ 2.2536e-01, -3.0062e-01,  1.0106e-01,  ..., -1.1121e+00,\n            2.6827e-01, -5.5333e-02],\n          [ 7.0521e-02, -3.1558e-01,  2.6445e-01,  ...,  1.1511e+00,\n            5.9738e-01, -1.5190e-01],\n          [-9.1370e-02,  1.0820e+00,  7.4802e-01,  ...,  1.9068e-01,\n           -4.5860e-01,  7.4459e-01],\n          ...,\n          [ 1.8985e-02,  1.2807e+00, -1.5447e+00,  ..., -6.0090e-01,\n            7.2441e-01,  8.2941e-01],\n          [ 3.5783e-01, -9.7884e-02,  1.9831e+00,  ..., -8.2288e-01,\n            3.5600e-01,  1.3519e-01],\n          [-3.1872e-01,  5.2968e-01, -1.1851e+00,  ..., -2.3451e-01,\n            6.3058e-01,  5.7916e-01]],\n\n         [[-1.9151e-01, -8.0938e-01, -7.4878e-01,  ...,  8.7690e-02,\n           -6.5998e-02, -8.7660e-02],\n          [-1.4635e-02,  4.4938e-01, -1.3227e+00,  ..., -1.3625e+00,\n           -1.4243e+00, -8.3266e-01],\n          [ 1.1510e+00,  5.9788e-01, -1.3344e+00,  ..., -2.9452e-01,\n           -1.1426e+00,  4.3165e-04],\n          ...,\n          [ 3.5482e-01,  2.9180e-01, -4.7351e-01,  ..., -1.0602e+00,\n           -9.5300e-01, -1.4889e-01],\n          [ 1.1904e+00,  3.3002e-01,  7.0450e-01,  ...,  5.8851e-01,\n           -1.3167e+00,  1.1598e+00],\n          [ 3.0639e-01,  2.4975e-01,  2.0160e-01,  ..., -6.6445e-01,\n            7.7924e-01,  3.1040e-01]]]], grad_fn=<ConvolutionBackward0>)\n                                      Trace Shapes:                        \n                                       Param Sites:                        \n             mvae$$$image_encoder.features.0.weight  32   3       3       3\n             mvae$$$image_encoder.features.2.weight  64  32       3       3\n             mvae$$$image_encoder.features.4.weight                      64\n               mvae$$$image_encoder.features.4.bias                      64\n             mvae$$$image_encoder.features.6.weight 128  64       3       3\n             mvae$$$image_encoder.features.8.weight                     128\n               mvae$$$image_encoder.features.8.bias                     128\n             mvae$$$image_encoder.features.9.weight 256 128       3       3\n            mvae$$$image_encoder.features.11.weight                     256\n              mvae$$$image_encoder.features.11.bias                     256\n          mvae$$$image_encoder.z_loc_layer.0.weight             512  262144\n            mvae$$$image_encoder.z_loc_layer.0.bias                     512\n          mvae$$$image_encoder.z_loc_layer.3.weight              50     512\n            mvae$$$image_encoder.z_loc_layer.3.bias                      50\n        mvae$$$image_encoder.z_scale_layer.0.weight             512  262144\n          mvae$$$image_encoder.z_scale_layer.0.bias                     512\n        mvae$$$image_encoder.z_scale_layer.3.weight              50     512\n          mvae$$$image_encoder.z_scale_layer.3.bias                      50\n             mvae$$$image_decoder.upsample.0.weight         4194304      50\n               mvae$$$image_decoder.upsample.0.bias                 4194304\n          mvae$$$image_decoder.hallucinate.0.weight 256 128       3       3\n          mvae$$$image_decoder.hallucinate.1.weight                     128\n            mvae$$$image_decoder.hallucinate.1.bias                     128\n          mvae$$$image_decoder.hallucinate.3.weight 128  64       3       3\n          mvae$$$image_decoder.hallucinate.4.weight                      64\n            mvae$$$image_decoder.hallucinate.4.bias                      64\n          mvae$$$image_decoder.hallucinate.6.weight  64  32       3       3\n          mvae$$$image_decoder.hallucinate.7.weight                      32\n            mvae$$$image_decoder.hallucinate.7.bias                      32\n          mvae$$$image_decoder.hallucinate.9.weight  32   3       3       3\n                  mvae$$$emotion_encoder.net.weight             512       8\n                    mvae$$$emotion_encoder.net.bias                     512\n        mvae$$$emotion_encoder.z_loc_layer.0.weight             512     512\n          mvae$$$emotion_encoder.z_loc_layer.0.bias                     512\n        mvae$$$emotion_encoder.z_loc_layer.2.weight              50     512\n          mvae$$$emotion_encoder.z_loc_layer.2.bias                      50\n      mvae$$$emotion_encoder.z_scale_layer.0.weight             512     512\n        mvae$$$emotion_encoder.z_scale_layer.0.bias                     512\n      mvae$$$emotion_encoder.z_scale_layer.2.weight              50     512\n        mvae$$$emotion_encoder.z_scale_layer.2.bias                      50\n                mvae$$$emotion_decoder.net.0.weight             512      50\n                  mvae$$$emotion_decoder.net.0.bias                     512\n  mvae$$$emotion_decoder.emotion_loc_layer.0.weight             512     512\n    mvae$$$emotion_decoder.emotion_loc_layer.0.bias                     512\n  mvae$$$emotion_decoder.emotion_loc_layer.2.weight               8     512\n    mvae$$$emotion_decoder.emotion_loc_layer.2.bias                       8\nmvae$$$emotion_decoder.emotion_scale_layer.0.weight             512     512\n  mvae$$$emotion_decoder.emotion_scale_layer.0.bias                     512\nmvae$$$emotion_decoder.emotion_scale_layer.2.weight               8     512\n  mvae$$$emotion_decoder.emotion_scale_layer.2.bias                       8\n                                      Sample Sites:                        \n                                             z dist       8      50       |\n                                              value       8      50       |",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/poutine/trace_messenger.py:174\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/poutine/messenger.py:12\u001b[0m, in \u001b[0;36m_context_wrap\u001b[0;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context:\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mMVAE.model\u001b[0;34m(self, images, emotions, annealing_beta)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage shape: \u001b[39m\u001b[38;5;124m'\u001b[39m, images\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 62\u001b[0m         pyro\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs_img\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBernoulli\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_loc\u001b[49m\u001b[43m)\u001b[49m, obs\u001b[38;5;241m=\u001b[39mimages)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# decode the latent code z (emotion decoder)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/distributions/distribution.py:18\u001b[0m, in \u001b[0;36mDistributionMeta.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/torch/distributions/bernoulli.py:48\u001b[0m, in \u001b[0;36mBernoulli.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     47\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mBernoulli\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/torch/distributions/distribution.py:55\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m---> 55\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     56\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28msuper\u001b[39m(Distribution, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter probs (Tensor of shape (8, 3, 128, 128)) of distribution Bernoulli(probs: torch.Size([8, 3, 128, 128])) to satisfy the constraint Interval(lower_bound=0.0, upper_bound=1.0), but found invalid values:\ntensor([[[[-3.9017e-02, -7.2188e-01,  2.9513e-01,  ...,  7.0698e-01,\n           -4.0400e-01,  1.1426e+00],\n          [ 9.4293e-01, -4.2566e-01, -1.4508e+00,  ...,  4.9947e-01,\n           -2.5893e-02,  2.5175e-01],\n          [-2.8492e-02,  8.6708e-01, -8.8784e-01,  ..., -3.9668e-01,\n           -3.1076e-01, -7.0688e-01],\n          ...,\n          [ 1.3940e+00, -3.6874e-01, -4.3644e-01,  ...,  5.4321e-01,\n            1.7384e-01, -2.0410e-01],\n          [-1.0094e+00,  5.3915e-01, -5.9737e-02,  ...,  1.2016e+00,\n            3.0995e-01, -2.8569e-01],\n          [ 4.6490e-01, -7.2065e-01, -1.7153e+00,  ...,  7.9064e-02,\n            3.3914e-01,  2.3666e-01]],\n\n         [[-2.7222e-01,  2.8789e-01,  2.0775e+00,  ...,  2.8436e-01,\n            3.6272e-01,  2.2622e-01],\n          [ 8.0380e-01, -1.9356e-01,  1.0928e+00,  ...,  2.8926e-01,\n            4.8483e-01, -5.4913e-02],\n          [-4.5173e-01,  1.3452e-01,  1.2322e+00,  ...,  2.3791e+00,\n           -8.5602e-02,  8.2655e-02],\n          ...,\n          [ 8.3299e-01, -2.5927e-01, -1.6366e+00,  ..., -8.3680e-02,\n           -4.8813e-01,  1.5064e+00],\n          [ 4.6731e-01, -2.6187e-01, -4.0458e-01,  ...,  1.1724e-01,\n           -1.0972e+00,  1.8863e-02],\n          [ 4.0596e-01, -2.4951e-01, -1.5650e+00,  ..., -4.2743e-01,\n           -5.7526e-02, -9.2211e-01]],\n\n         [[ 9.8827e-02,  3.6070e-01, -7.6037e-01,  ..., -7.8843e-01,\n            1.2897e+00,  1.6489e-01],\n          [ 3.5030e-01,  6.3007e-01,  3.2396e-01,  ...,  1.2788e-01,\n            6.0136e-02,  7.2336e-01],\n          [-4.6332e-01,  4.1395e-01, -4.8604e-01,  ..., -5.4677e-02,\n           -1.0098e+00,  4.1479e-01],\n          ...,\n          [-4.2682e-01,  8.7209e-01,  3.5469e-01,  ..., -5.6740e-01,\n           -5.8024e-01, -1.1075e+00],\n          [ 7.4761e-02,  1.1156e+00,  6.0063e-01,  ..., -4.3437e-01,\n           -5.6222e-01,  7.8487e-02],\n          [-9.6210e-02,  2.1367e+00,  2.4944e-01,  ...,  5.7961e-01,\n           -8.0608e-01,  4.8292e-01]]],\n\n\n        [[[ 2.5746e-02, -6.8964e-01,  3.0840e-01,  ...,  1.6439e-01,\n            5.7820e-01,  8.0057e-01],\n          [-1.6396e-01, -4.4783e-01,  7.2353e-01,  ...,  1.2401e-01,\n            1.4810e+00, -3.7449e-01],\n          [-3.9761e-01,  9.5291e-01, -5.1215e-02,  ..., -6.3679e-01,\n            1.1281e+00,  8.2581e-01],\n          ...,\n          [-1.9110e-02,  7.2256e-01,  3.0355e-01,  ..., -3.8227e-01,\n           -3.8176e-01, -5.8665e-01],\n          [ 2.3904e-01,  3.2932e-01,  2.6372e-01,  ...,  5.6213e-01,\n            8.7643e-01,  3.2533e-01],\n          [ 7.6803e-01,  6.2389e-01, -3.6977e-01,  ..., -3.1811e-03,\n            6.8925e-01, -1.6380e-01]],\n\n         [[ 4.5422e-01, -1.2126e-01,  1.7723e-03,  ...,  1.4019e-01,\n           -2.3005e-01,  8.7588e-02],\n          [-1.1691e-01,  6.0980e-01,  6.2077e-01,  ...,  3.9957e-02,\n            5.8375e-01,  5.4512e-01],\n          [ 9.6048e-02,  3.3861e-01, -1.2552e-01,  ..., -8.0926e-01,\n            1.1439e+00,  8.8580e-01],\n          ...,\n          [ 4.9670e-01, -5.3180e-01,  1.0598e+00,  ..., -4.0450e-01,\n            9.5854e-01,  2.3169e-01],\n          [-4.2756e-01,  1.3666e-01, -3.2273e-01,  ...,  4.3074e-01,\n            5.4341e-01, -1.0114e-01],\n          [ 5.4346e-02,  1.0331e+00, -1.1682e-01,  ..., -6.9097e-01,\n           -2.8787e-02, -2.3072e-02]],\n\n         [[-2.0442e-01,  2.5657e-01, -3.3531e-01,  ...,  1.2308e-01,\n           -6.5697e-01,  2.2010e-02],\n          [-8.1277e-01, -1.3558e+00, -2.1419e-01,  ..., -9.4393e-01,\n           -8.5543e-01, -9.2960e-01],\n          [ 4.6743e-01,  2.6072e-01,  2.9905e-01,  ..., -1.7642e-02,\n           -2.4371e+00,  2.9656e-02],\n          ...,\n          [-6.4899e-01,  1.1591e+00,  1.5636e-01,  ..., -1.1162e+00,\n           -6.4924e-01,  9.2095e-02],\n          [ 3.9833e-01,  3.8702e-01, -1.1339e-01,  ..., -1.6524e-01,\n           -1.2188e+00,  7.1567e-01],\n          [-1.3767e-01, -1.6588e-01, -4.0546e-01,  ...,  1.0771e-01,\n            7.1223e-01, -4.5778e-01]]],\n\n\n        [[[ 9.9965e-01, -3.0301e-01,  8.8343e-02,  ..., -3.1927e-01,\n            5.9005e-01,  1.8413e-01],\n          [ 6.5223e-01, -4.7682e-01, -1.1774e+00,  ..., -3.5193e-02,\n           -1.1047e+00, -1.2156e+00],\n          [-9.2792e-01,  1.3876e-01,  1.2122e-01,  ...,  8.2273e-01,\n           -3.3880e-01,  1.3593e-01],\n          ...,\n          [ 8.6760e-02,  1.7300e+00, -4.2076e-01,  ..., -2.6218e+00,\n            1.0819e-01,  6.8241e-01],\n          [ 1.0043e+00,  8.6387e-01,  1.7983e+00,  ..., -8.7246e-01,\n            2.0311e+00,  1.0811e+00],\n          [ 6.1507e-01, -2.8476e-02, -1.0656e+00,  ..., -5.1055e-01,\n           -7.3141e-01, -5.9831e-01]],\n\n         [[ 4.7994e-01, -7.9971e-02,  3.6339e-01,  ...,  1.3162e-02,\n            1.8769e-01,  3.3493e-02],\n          [-5.1917e-01,  4.4698e-01, -6.3489e-01,  ...,  5.7594e-01,\n            2.7531e-01,  7.4175e-01],\n          [-1.5960e-01, -1.2524e+00, -5.0536e-01,  ...,  7.3972e-01,\n            1.3050e+00,  1.4190e+00],\n          ...,\n          [ 5.7906e-01, -3.1778e-01, -7.4224e-01,  ...,  7.5666e-01,\n           -4.8276e-01,  1.7860e+00],\n          [ 9.1892e-03,  2.0764e-01,  8.6349e-03,  ..., -1.0164e-01,\n            8.2697e-01,  3.9829e-01],\n          [ 1.7038e-01,  4.6167e-01, -2.0694e-01,  ..., -7.7880e-01,\n            5.9491e-01,  3.4319e-01]],\n\n         [[ 1.0492e-01, -1.8227e-01, -1.8404e-02,  ...,  6.7217e-01,\n           -7.3851e-01, -2.3027e-01],\n          [-1.1986e-01, -1.8996e-01,  1.9871e-01,  ...,  8.3738e-01,\n           -9.8994e-01, -6.1400e-01],\n          [-1.0485e+00,  8.9330e-01,  2.2865e-01,  ...,  3.4802e-01,\n           -8.6168e-01,  1.1340e+00],\n          ...,\n          [-2.3495e-01,  1.9848e-01,  8.0362e-01,  ...,  1.1575e+00,\n           -1.2087e+00, -2.8792e-01],\n          [ 9.2640e-01,  1.1055e+00,  2.6067e-01,  ..., -1.1524e+00,\n            1.1371e+00, -8.4982e-01],\n          [ 6.1988e-01,  3.4533e-02,  3.1822e-01,  ...,  7.8782e-01,\n           -6.0970e-01, -4.8516e-01]]],\n\n\n        ...,\n\n\n        [[[-1.4784e-01,  2.7171e-01, -3.8212e-01,  ...,  1.0399e-01,\n           -2.5261e-01,  9.2444e-02],\n          [ 1.7680e-01, -6.6754e-01, -7.7115e-02,  ...,  2.1930e+00,\n            2.3621e-01,  1.1916e+00],\n          [-2.6279e-02, -4.1784e-01, -8.3798e-01,  ...,  8.0678e-02,\n            7.1473e-01,  2.9554e-01],\n          ...,\n          [ 9.0389e-01,  1.9255e-01, -1.5749e-01,  ..., -5.9989e-01,\n            1.2586e+00, -6.7517e-01],\n          [ 2.7228e-01,  1.6775e+00,  7.5060e-01,  ...,  1.6830e+00,\n           -3.8423e-01,  1.4338e-01],\n          [ 2.4646e-01, -3.8633e-01, -7.3344e-02,  ...,  1.1645e+00,\n            4.7968e-01, -1.2715e-01]],\n\n         [[ 4.3851e-02,  2.6447e-01,  1.1777e-01,  ..., -3.8761e-02,\n           -2.0988e-01, -4.7920e-02],\n          [ 3.9321e-01,  2.6125e-01,  1.8839e-01,  ...,  5.7798e-01,\n            3.9192e-01,  6.7977e-01],\n          [ 4.0889e-01,  3.9318e-01,  2.7429e-01,  ..., -1.9893e-01,\n            1.2501e+00, -4.4598e-01],\n          ...,\n          [-7.0468e-01,  1.1383e+00, -9.6469e-01,  ...,  1.2142e+00,\n           -5.5559e-01,  6.0136e-01],\n          [-4.2415e-01,  7.2701e-01,  2.7348e-01,  ...,  9.7252e-01,\n            5.3545e-02,  8.9845e-01],\n          [-3.5599e-02, -9.9147e-01,  1.0962e-01,  ..., -4.2500e-01,\n            5.9879e-03,  6.6595e-02]],\n\n         [[ 1.3414e-01,  1.5417e-01,  8.2356e-01,  ..., -6.4451e-01,\n           -8.2499e-01, -1.1627e-01],\n          [ 4.8968e-01,  6.5762e-01,  1.0189e-01,  ..., -5.5968e-01,\n           -1.2620e+00, -5.7268e-01],\n          [-5.3173e-01, -4.4683e-02,  1.6379e-01,  ..., -1.4492e+00,\n           -3.4923e-01,  1.3164e-01],\n          ...,\n          [-3.4171e-01, -6.5994e-01, -2.3982e-01,  ...,  8.3372e-01,\n           -1.5697e-02, -1.0999e+00],\n          [-8.5438e-01,  7.9687e-01,  1.7074e-01,  ...,  4.1932e-01,\n            4.3002e-01, -5.2091e-01],\n          [ 1.2794e-01,  7.3050e-01,  4.6818e-01,  ..., -2.1569e-01,\n           -1.1805e-01, -6.0279e-02]]],\n\n\n        [[[-2.6010e-01,  3.3723e-01, -2.1189e-01,  ...,  5.4050e-01,\n            1.1406e+00, -2.3828e-02],\n          [ 3.8747e-01, -3.9276e-01, -6.5514e-01,  ...,  4.9900e-02,\n           -2.9840e-02,  1.8531e-02],\n          [ 7.1674e-01,  2.7109e-01, -1.4508e+00,  ..., -2.6001e+00,\n           -1.2457e-01, -3.4741e-01],\n          ...,\n          [ 6.7617e-01,  7.4334e-01,  8.9434e-01,  ...,  9.9241e-01,\n            5.7569e-01,  3.0964e-01],\n          [ 3.0716e-01,  9.5590e-01,  1.5069e+00,  ...,  1.3535e+00,\n           -5.4177e-01, -3.1954e-01],\n          [-5.0877e-01, -1.0374e+00,  1.2553e-01,  ..., -3.2210e-01,\n            5.3504e-01, -4.4945e-01]],\n\n         [[-6.6355e-01,  1.5975e-02, -1.1028e+00,  ...,  4.2119e-01,\n            1.4300e-01, -1.0500e+00],\n          [ 2.9872e-02,  8.8631e-01, -6.1298e-01,  ...,  7.4856e-01,\n           -2.5026e-01, -6.9316e-01],\n          [-3.7613e-01, -1.6566e+00, -4.7379e-01,  ...,  1.3844e+00,\n           -2.1917e-01, -3.4775e-02],\n          ...,\n          [ 6.4191e-03, -6.4329e-01, -3.4590e-01,  ...,  7.1099e-01,\n           -8.6027e-01,  1.9707e+00],\n          [ 4.9977e-01,  8.7028e-01, -1.2418e+00,  ...,  4.7148e-02,\n           -2.1899e+00,  5.0688e-02],\n          [-4.3150e-01, -9.5305e-01,  6.0477e-03,  ...,  4.6361e-02,\n            1.2867e+00, -5.2287e-01]],\n\n         [[ 2.1083e-01, -3.4744e-01, -8.8930e-01,  ..., -1.5877e+00,\n            7.6081e-01,  8.1642e-01],\n          [-7.2559e-01,  1.0759e-03,  5.0080e-01,  ..., -9.0819e-01,\n           -1.3664e+00, -5.0009e-01],\n          [ 9.8828e-01,  2.0458e-01, -3.2092e-01,  ..., -1.0505e+00,\n            7.6977e-01, -1.0954e+00],\n          ...,\n          [ 3.1315e-01, -1.8013e-01,  6.6712e-03,  ..., -3.4387e-01,\n            6.7799e-02, -7.9891e-01],\n          [ 9.3795e-01,  8.4810e-01, -3.3065e-02,  ..., -1.4951e+00,\n            1.8487e-01, -7.2444e-01],\n          [-2.9354e-01,  4.9934e-01,  1.5927e+00,  ...,  6.8559e-01,\n           -5.8828e-02,  7.8406e-02]]],\n\n\n        [[[-2.7788e-01, -5.9287e-01, -1.0850e+00,  ...,  1.1521e-01,\n           -1.0350e-01, -1.0359e-01],\n          [ 9.7773e-02, -6.6024e-01,  1.3205e+00,  ...,  5.3065e-01,\n            1.4022e+00, -1.3554e+00],\n          [ 1.2476e-01, -3.5420e-01, -7.5356e-01,  ..., -1.4845e+00,\n           -8.2950e-01, -3.1953e-01],\n          ...,\n          [-2.2498e-01,  1.1432e+00, -6.3102e-02,  ...,  2.2542e+00,\n           -4.3919e-02, -4.5381e-01],\n          [-1.1767e-01,  3.8476e-01,  2.8890e-01,  ...,  5.0568e-01,\n            1.9877e+00, -4.0198e-01],\n          [ 8.3461e-01, -3.8473e-01, -6.2184e-01,  ...,  1.2079e-01,\n            1.2926e+00, -9.2583e-01]],\n\n         [[ 2.2536e-01, -3.0062e-01,  1.0106e-01,  ..., -1.1121e+00,\n            2.6827e-01, -5.5333e-02],\n          [ 7.0521e-02, -3.1558e-01,  2.6445e-01,  ...,  1.1511e+00,\n            5.9738e-01, -1.5190e-01],\n          [-9.1370e-02,  1.0820e+00,  7.4802e-01,  ...,  1.9068e-01,\n           -4.5860e-01,  7.4459e-01],\n          ...,\n          [ 1.8985e-02,  1.2807e+00, -1.5447e+00,  ..., -6.0090e-01,\n            7.2441e-01,  8.2941e-01],\n          [ 3.5783e-01, -9.7884e-02,  1.9831e+00,  ..., -8.2288e-01,\n            3.5600e-01,  1.3519e-01],\n          [-3.1872e-01,  5.2968e-01, -1.1851e+00,  ..., -2.3451e-01,\n            6.3058e-01,  5.7916e-01]],\n\n         [[-1.9151e-01, -8.0938e-01, -7.4878e-01,  ...,  8.7690e-02,\n           -6.5998e-02, -8.7660e-02],\n          [-1.4635e-02,  4.4938e-01, -1.3227e+00,  ..., -1.3625e+00,\n           -1.4243e+00, -8.3266e-01],\n          [ 1.1510e+00,  5.9788e-01, -1.3344e+00,  ..., -2.9452e-01,\n           -1.1426e+00,  4.3165e-04],\n          ...,\n          [ 3.5482e-01,  2.9180e-01, -4.7351e-01,  ..., -1.0602e+00,\n           -9.5300e-01, -1.4889e-01],\n          [ 1.1904e+00,  3.3002e-01,  7.0450e-01,  ...,  5.8851e-01,\n           -1.3167e+00,  1.1598e+00],\n          [ 3.0639e-01,  2.4975e-01,  2.0160e-01,  ..., -6.6445e-01,\n            7.7924e-01,  3.1040e-01]]]], grad_fn=<ConvolutionBackward0>)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     faces, ratings, outcomes \u001b[38;5;241m=\u001b[39m faces\u001b[38;5;241m.\u001b[39mcuda(), ratings\u001b[38;5;241m.\u001b[39mcuda(), outcomes\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# do ELBO gradient and accumulate loss\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#print(\"Batch: \", batch_num, \"out of\", len(train_loader))\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43msvi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfaces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memotions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memotions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m svi\u001b[38;5;241m.\u001b[39mstep(images\u001b[38;5;241m=\u001b[39mfaces, emotions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m svi\u001b[38;5;241m.\u001b[39mstep(images\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, emotions\u001b[38;5;241m=\u001b[39memotions)\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/infer/svi.py:145\u001b[0m, in \u001b[0;36mSVI.step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# get loss and compute gradients\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m poutine\u001b[38;5;241m.\u001b[39mtrace(param_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m param_capture:\n\u001b[0;32m--> 145\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_and_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m    148\u001b[0m     site[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munconstrained() \u001b[38;5;28;01mfor\u001b[39;00m site \u001b[38;5;129;01min\u001b[39;00m param_capture\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    149\u001b[0m )\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# actually perform gradient steps\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# torch.optim objects gets instantiated for any params that haven't been seen yet\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/infer/trace_elbo.py:140\u001b[0m, in \u001b[0;36mTrace_ELBO.loss_and_grads\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# grab a trace from the generator\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_trace, guide_trace \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_traces(model, guide, args, kwargs):\n\u001b[1;32m    141\u001b[0m     loss_particle, surrogate_loss_particle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_differentiable_loss_particle(\n\u001b[1;32m    142\u001b[0m         model_trace, guide_trace\n\u001b[1;32m    143\u001b[0m     )\n\u001b[1;32m    144\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_particle \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_particles\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/infer/elbo.py:182\u001b[0m, in \u001b[0;36mELBO._get_traces\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_particles):\n\u001b[0;32m--> 182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/infer/trace_elbo.py:57\u001b[0m, in \u001b[0;36mTrace_ELBO._get_trace\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, guide, args, kwargs):\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m    Returns a single trace from the guide, and the model that is run\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    against it.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     model_trace, guide_trace \u001b[38;5;241m=\u001b[39m \u001b[43mget_importance_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_plate_nesting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_validation_enabled():\n\u001b[1;32m     61\u001b[0m         check_if_enumerated(guide_trace)\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/infer/enum.py:65\u001b[0m, in \u001b[0;36mget_importance_trace\u001b[0;34m(graph_type, max_plate_nesting, model, guide, args, kwargs, detach)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m detach:\n\u001b[1;32m     64\u001b[0m         guide_trace\u001b[38;5;241m.\u001b[39mdetach_()\n\u001b[0;32m---> 65\u001b[0m     model_trace \u001b[38;5;241m=\u001b[39m \u001b[43mpoutine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpoutine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguide_trace\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_type\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_validation_enabled():\n\u001b[1;32m     70\u001b[0m     check_model_guide_match(model_trace, guide_trace, max_plate_nesting)\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/poutine/trace_messenger.py:198\u001b[0m, in \u001b[0;36mTraceHandler.get_trace\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m    :returns: data structure\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m    :rtype: pyro.poutine.Trace\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m    Calls this poutine and returns its trace instead of the function's return value.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsngr\u001b[38;5;241m.\u001b[39mget_trace()\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/poutine/trace_messenger.py:180\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m         exc \u001b[38;5;241m=\u001b[39m exc_type(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(exc_value, shapes))\n\u001b[1;32m    179\u001b[0m         exc \u001b[38;5;241m=\u001b[39m exc\u001b[38;5;241m.\u001b[39mwith_traceback(traceback)\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsngr\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39madd_node(\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_RETURN\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_RETURN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, value\u001b[38;5;241m=\u001b[39mret\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/poutine/trace_messenger.py:174\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsngr\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39madd_node(\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_INPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_INPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    172\u001b[0m )\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    176\u001b[0m     exc_type, exc_value, traceback \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/poutine/messenger.py:12\u001b[0m, in \u001b[0;36m_context_wrap\u001b[0;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_context_wrap\u001b[39m(context, fn, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m context:\n\u001b[0;32m---> 12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mMVAE.model\u001b[0;34m(self, images, emotions, annealing_beta)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage loc: \u001b[39m\u001b[38;5;124m'\u001b[39m, img_loc\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage shape: \u001b[39m\u001b[38;5;124m'\u001b[39m, images\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 62\u001b[0m         pyro\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs_img\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBernoulli\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_loc\u001b[49m\u001b[43m)\u001b[49m, obs\u001b[38;5;241m=\u001b[39mimages)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# decode the latent code z (emotion decoder)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m emotion_loc, emotion_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memotion_decoder\u001b[38;5;241m.\u001b[39mforward(z)\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/distributions/distribution.py:18\u001b[0m, in \u001b[0;36mDistributionMeta.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/torch/distributions/bernoulli.py:48\u001b[0m, in \u001b[0;36mBernoulli.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mBernoulli\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/torch/distributions/distribution.py:55\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     53\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m---> 55\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     56\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28msuper\u001b[39m(Distribution, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter probs (Tensor of shape (8, 3, 128, 128)) of distribution Bernoulli(probs: torch.Size([8, 3, 128, 128])) to satisfy the constraint Interval(lower_bound=0.0, upper_bound=1.0), but found invalid values:\ntensor([[[[-3.9017e-02, -7.2188e-01,  2.9513e-01,  ...,  7.0698e-01,\n           -4.0400e-01,  1.1426e+00],\n          [ 9.4293e-01, -4.2566e-01, -1.4508e+00,  ...,  4.9947e-01,\n           -2.5893e-02,  2.5175e-01],\n          [-2.8492e-02,  8.6708e-01, -8.8784e-01,  ..., -3.9668e-01,\n           -3.1076e-01, -7.0688e-01],\n          ...,\n          [ 1.3940e+00, -3.6874e-01, -4.3644e-01,  ...,  5.4321e-01,\n            1.7384e-01, -2.0410e-01],\n          [-1.0094e+00,  5.3915e-01, -5.9737e-02,  ...,  1.2016e+00,\n            3.0995e-01, -2.8569e-01],\n          [ 4.6490e-01, -7.2065e-01, -1.7153e+00,  ...,  7.9064e-02,\n            3.3914e-01,  2.3666e-01]],\n\n         [[-2.7222e-01,  2.8789e-01,  2.0775e+00,  ...,  2.8436e-01,\n            3.6272e-01,  2.2622e-01],\n          [ 8.0380e-01, -1.9356e-01,  1.0928e+00,  ...,  2.8926e-01,\n            4.8483e-01, -5.4913e-02],\n          [-4.5173e-01,  1.3452e-01,  1.2322e+00,  ...,  2.3791e+00,\n           -8.5602e-02,  8.2655e-02],\n          ...,\n          [ 8.3299e-01, -2.5927e-01, -1.6366e+00,  ..., -8.3680e-02,\n           -4.8813e-01,  1.5064e+00],\n          [ 4.6731e-01, -2.6187e-01, -4.0458e-01,  ...,  1.1724e-01,\n           -1.0972e+00,  1.8863e-02],\n          [ 4.0596e-01, -2.4951e-01, -1.5650e+00,  ..., -4.2743e-01,\n           -5.7526e-02, -9.2211e-01]],\n\n         [[ 9.8827e-02,  3.6070e-01, -7.6037e-01,  ..., -7.8843e-01,\n            1.2897e+00,  1.6489e-01],\n          [ 3.5030e-01,  6.3007e-01,  3.2396e-01,  ...,  1.2788e-01,\n            6.0136e-02,  7.2336e-01],\n          [-4.6332e-01,  4.1395e-01, -4.8604e-01,  ..., -5.4677e-02,\n           -1.0098e+00,  4.1479e-01],\n          ...,\n          [-4.2682e-01,  8.7209e-01,  3.5469e-01,  ..., -5.6740e-01,\n           -5.8024e-01, -1.1075e+00],\n          [ 7.4761e-02,  1.1156e+00,  6.0063e-01,  ..., -4.3437e-01,\n           -5.6222e-01,  7.8487e-02],\n          [-9.6210e-02,  2.1367e+00,  2.4944e-01,  ...,  5.7961e-01,\n           -8.0608e-01,  4.8292e-01]]],\n\n\n        [[[ 2.5746e-02, -6.8964e-01,  3.0840e-01,  ...,  1.6439e-01,\n            5.7820e-01,  8.0057e-01],\n          [-1.6396e-01, -4.4783e-01,  7.2353e-01,  ...,  1.2401e-01,\n            1.4810e+00, -3.7449e-01],\n          [-3.9761e-01,  9.5291e-01, -5.1215e-02,  ..., -6.3679e-01,\n            1.1281e+00,  8.2581e-01],\n          ...,\n          [-1.9110e-02,  7.2256e-01,  3.0355e-01,  ..., -3.8227e-01,\n           -3.8176e-01, -5.8665e-01],\n          [ 2.3904e-01,  3.2932e-01,  2.6372e-01,  ...,  5.6213e-01,\n            8.7643e-01,  3.2533e-01],\n          [ 7.6803e-01,  6.2389e-01, -3.6977e-01,  ..., -3.1811e-03,\n            6.8925e-01, -1.6380e-01]],\n\n         [[ 4.5422e-01, -1.2126e-01,  1.7723e-03,  ...,  1.4019e-01,\n           -2.3005e-01,  8.7588e-02],\n          [-1.1691e-01,  6.0980e-01,  6.2077e-01,  ...,  3.9957e-02,\n            5.8375e-01,  5.4512e-01],\n          [ 9.6048e-02,  3.3861e-01, -1.2552e-01,  ..., -8.0926e-01,\n            1.1439e+00,  8.8580e-01],\n          ...,\n          [ 4.9670e-01, -5.3180e-01,  1.0598e+00,  ..., -4.0450e-01,\n            9.5854e-01,  2.3169e-01],\n          [-4.2756e-01,  1.3666e-01, -3.2273e-01,  ...,  4.3074e-01,\n            5.4341e-01, -1.0114e-01],\n          [ 5.4346e-02,  1.0331e+00, -1.1682e-01,  ..., -6.9097e-01,\n           -2.8787e-02, -2.3072e-02]],\n\n         [[-2.0442e-01,  2.5657e-01, -3.3531e-01,  ...,  1.2308e-01,\n           -6.5697e-01,  2.2010e-02],\n          [-8.1277e-01, -1.3558e+00, -2.1419e-01,  ..., -9.4393e-01,\n           -8.5543e-01, -9.2960e-01],\n          [ 4.6743e-01,  2.6072e-01,  2.9905e-01,  ..., -1.7642e-02,\n           -2.4371e+00,  2.9656e-02],\n          ...,\n          [-6.4899e-01,  1.1591e+00,  1.5636e-01,  ..., -1.1162e+00,\n           -6.4924e-01,  9.2095e-02],\n          [ 3.9833e-01,  3.8702e-01, -1.1339e-01,  ..., -1.6524e-01,\n           -1.2188e+00,  7.1567e-01],\n          [-1.3767e-01, -1.6588e-01, -4.0546e-01,  ...,  1.0771e-01,\n            7.1223e-01, -4.5778e-01]]],\n\n\n        [[[ 9.9965e-01, -3.0301e-01,  8.8343e-02,  ..., -3.1927e-01,\n            5.9005e-01,  1.8413e-01],\n          [ 6.5223e-01, -4.7682e-01, -1.1774e+00,  ..., -3.5193e-02,\n           -1.1047e+00, -1.2156e+00],\n          [-9.2792e-01,  1.3876e-01,  1.2122e-01,  ...,  8.2273e-01,\n           -3.3880e-01,  1.3593e-01],\n          ...,\n          [ 8.6760e-02,  1.7300e+00, -4.2076e-01,  ..., -2.6218e+00,\n            1.0819e-01,  6.8241e-01],\n          [ 1.0043e+00,  8.6387e-01,  1.7983e+00,  ..., -8.7246e-01,\n            2.0311e+00,  1.0811e+00],\n          [ 6.1507e-01, -2.8476e-02, -1.0656e+00,  ..., -5.1055e-01,\n           -7.3141e-01, -5.9831e-01]],\n\n         [[ 4.7994e-01, -7.9971e-02,  3.6339e-01,  ...,  1.3162e-02,\n            1.8769e-01,  3.3493e-02],\n          [-5.1917e-01,  4.4698e-01, -6.3489e-01,  ...,  5.7594e-01,\n            2.7531e-01,  7.4175e-01],\n          [-1.5960e-01, -1.2524e+00, -5.0536e-01,  ...,  7.3972e-01,\n            1.3050e+00,  1.4190e+00],\n          ...,\n          [ 5.7906e-01, -3.1778e-01, -7.4224e-01,  ...,  7.5666e-01,\n           -4.8276e-01,  1.7860e+00],\n          [ 9.1892e-03,  2.0764e-01,  8.6349e-03,  ..., -1.0164e-01,\n            8.2697e-01,  3.9829e-01],\n          [ 1.7038e-01,  4.6167e-01, -2.0694e-01,  ..., -7.7880e-01,\n            5.9491e-01,  3.4319e-01]],\n\n         [[ 1.0492e-01, -1.8227e-01, -1.8404e-02,  ...,  6.7217e-01,\n           -7.3851e-01, -2.3027e-01],\n          [-1.1986e-01, -1.8996e-01,  1.9871e-01,  ...,  8.3738e-01,\n           -9.8994e-01, -6.1400e-01],\n          [-1.0485e+00,  8.9330e-01,  2.2865e-01,  ...,  3.4802e-01,\n           -8.6168e-01,  1.1340e+00],\n          ...,\n          [-2.3495e-01,  1.9848e-01,  8.0362e-01,  ...,  1.1575e+00,\n           -1.2087e+00, -2.8792e-01],\n          [ 9.2640e-01,  1.1055e+00,  2.6067e-01,  ..., -1.1524e+00,\n            1.1371e+00, -8.4982e-01],\n          [ 6.1988e-01,  3.4533e-02,  3.1822e-01,  ...,  7.8782e-01,\n           -6.0970e-01, -4.8516e-01]]],\n\n\n        ...,\n\n\n        [[[-1.4784e-01,  2.7171e-01, -3.8212e-01,  ...,  1.0399e-01,\n           -2.5261e-01,  9.2444e-02],\n          [ 1.7680e-01, -6.6754e-01, -7.7115e-02,  ...,  2.1930e+00,\n            2.3621e-01,  1.1916e+00],\n          [-2.6279e-02, -4.1784e-01, -8.3798e-01,  ...,  8.0678e-02,\n            7.1473e-01,  2.9554e-01],\n          ...,\n          [ 9.0389e-01,  1.9255e-01, -1.5749e-01,  ..., -5.9989e-01,\n            1.2586e+00, -6.7517e-01],\n          [ 2.7228e-01,  1.6775e+00,  7.5060e-01,  ...,  1.6830e+00,\n           -3.8423e-01,  1.4338e-01],\n          [ 2.4646e-01, -3.8633e-01, -7.3344e-02,  ...,  1.1645e+00,\n            4.7968e-01, -1.2715e-01]],\n\n         [[ 4.3851e-02,  2.6447e-01,  1.1777e-01,  ..., -3.8761e-02,\n           -2.0988e-01, -4.7920e-02],\n          [ 3.9321e-01,  2.6125e-01,  1.8839e-01,  ...,  5.7798e-01,\n            3.9192e-01,  6.7977e-01],\n          [ 4.0889e-01,  3.9318e-01,  2.7429e-01,  ..., -1.9893e-01,\n            1.2501e+00, -4.4598e-01],\n          ...,\n          [-7.0468e-01,  1.1383e+00, -9.6469e-01,  ...,  1.2142e+00,\n           -5.5559e-01,  6.0136e-01],\n          [-4.2415e-01,  7.2701e-01,  2.7348e-01,  ...,  9.7252e-01,\n            5.3545e-02,  8.9845e-01],\n          [-3.5599e-02, -9.9147e-01,  1.0962e-01,  ..., -4.2500e-01,\n            5.9879e-03,  6.6595e-02]],\n\n         [[ 1.3414e-01,  1.5417e-01,  8.2356e-01,  ..., -6.4451e-01,\n           -8.2499e-01, -1.1627e-01],\n          [ 4.8968e-01,  6.5762e-01,  1.0189e-01,  ..., -5.5968e-01,\n           -1.2620e+00, -5.7268e-01],\n          [-5.3173e-01, -4.4683e-02,  1.6379e-01,  ..., -1.4492e+00,\n           -3.4923e-01,  1.3164e-01],\n          ...,\n          [-3.4171e-01, -6.5994e-01, -2.3982e-01,  ...,  8.3372e-01,\n           -1.5697e-02, -1.0999e+00],\n          [-8.5438e-01,  7.9687e-01,  1.7074e-01,  ...,  4.1932e-01,\n            4.3002e-01, -5.2091e-01],\n          [ 1.2794e-01,  7.3050e-01,  4.6818e-01,  ..., -2.1569e-01,\n           -1.1805e-01, -6.0279e-02]]],\n\n\n        [[[-2.6010e-01,  3.3723e-01, -2.1189e-01,  ...,  5.4050e-01,\n            1.1406e+00, -2.3828e-02],\n          [ 3.8747e-01, -3.9276e-01, -6.5514e-01,  ...,  4.9900e-02,\n           -2.9840e-02,  1.8531e-02],\n          [ 7.1674e-01,  2.7109e-01, -1.4508e+00,  ..., -2.6001e+00,\n           -1.2457e-01, -3.4741e-01],\n          ...,\n          [ 6.7617e-01,  7.4334e-01,  8.9434e-01,  ...,  9.9241e-01,\n            5.7569e-01,  3.0964e-01],\n          [ 3.0716e-01,  9.5590e-01,  1.5069e+00,  ...,  1.3535e+00,\n           -5.4177e-01, -3.1954e-01],\n          [-5.0877e-01, -1.0374e+00,  1.2553e-01,  ..., -3.2210e-01,\n            5.3504e-01, -4.4945e-01]],\n\n         [[-6.6355e-01,  1.5975e-02, -1.1028e+00,  ...,  4.2119e-01,\n            1.4300e-01, -1.0500e+00],\n          [ 2.9872e-02,  8.8631e-01, -6.1298e-01,  ...,  7.4856e-01,\n           -2.5026e-01, -6.9316e-01],\n          [-3.7613e-01, -1.6566e+00, -4.7379e-01,  ...,  1.3844e+00,\n           -2.1917e-01, -3.4775e-02],\n          ...,\n          [ 6.4191e-03, -6.4329e-01, -3.4590e-01,  ...,  7.1099e-01,\n           -8.6027e-01,  1.9707e+00],\n          [ 4.9977e-01,  8.7028e-01, -1.2418e+00,  ...,  4.7148e-02,\n           -2.1899e+00,  5.0688e-02],\n          [-4.3150e-01, -9.5305e-01,  6.0477e-03,  ...,  4.6361e-02,\n            1.2867e+00, -5.2287e-01]],\n\n         [[ 2.1083e-01, -3.4744e-01, -8.8930e-01,  ..., -1.5877e+00,\n            7.6081e-01,  8.1642e-01],\n          [-7.2559e-01,  1.0759e-03,  5.0080e-01,  ..., -9.0819e-01,\n           -1.3664e+00, -5.0009e-01],\n          [ 9.8828e-01,  2.0458e-01, -3.2092e-01,  ..., -1.0505e+00,\n            7.6977e-01, -1.0954e+00],\n          ...,\n          [ 3.1315e-01, -1.8013e-01,  6.6712e-03,  ..., -3.4387e-01,\n            6.7799e-02, -7.9891e-01],\n          [ 9.3795e-01,  8.4810e-01, -3.3065e-02,  ..., -1.4951e+00,\n            1.8487e-01, -7.2444e-01],\n          [-2.9354e-01,  4.9934e-01,  1.5927e+00,  ...,  6.8559e-01,\n           -5.8828e-02,  7.8406e-02]]],\n\n\n        [[[-2.7788e-01, -5.9287e-01, -1.0850e+00,  ...,  1.1521e-01,\n           -1.0350e-01, -1.0359e-01],\n          [ 9.7773e-02, -6.6024e-01,  1.3205e+00,  ...,  5.3065e-01,\n            1.4022e+00, -1.3554e+00],\n          [ 1.2476e-01, -3.5420e-01, -7.5356e-01,  ..., -1.4845e+00,\n           -8.2950e-01, -3.1953e-01],\n          ...,\n          [-2.2498e-01,  1.1432e+00, -6.3102e-02,  ...,  2.2542e+00,\n           -4.3919e-02, -4.5381e-01],\n          [-1.1767e-01,  3.8476e-01,  2.8890e-01,  ...,  5.0568e-01,\n            1.9877e+00, -4.0198e-01],\n          [ 8.3461e-01, -3.8473e-01, -6.2184e-01,  ...,  1.2079e-01,\n            1.2926e+00, -9.2583e-01]],\n\n         [[ 2.2536e-01, -3.0062e-01,  1.0106e-01,  ..., -1.1121e+00,\n            2.6827e-01, -5.5333e-02],\n          [ 7.0521e-02, -3.1558e-01,  2.6445e-01,  ...,  1.1511e+00,\n            5.9738e-01, -1.5190e-01],\n          [-9.1370e-02,  1.0820e+00,  7.4802e-01,  ...,  1.9068e-01,\n           -4.5860e-01,  7.4459e-01],\n          ...,\n          [ 1.8985e-02,  1.2807e+00, -1.5447e+00,  ..., -6.0090e-01,\n            7.2441e-01,  8.2941e-01],\n          [ 3.5783e-01, -9.7884e-02,  1.9831e+00,  ..., -8.2288e-01,\n            3.5600e-01,  1.3519e-01],\n          [-3.1872e-01,  5.2968e-01, -1.1851e+00,  ..., -2.3451e-01,\n            6.3058e-01,  5.7916e-01]],\n\n         [[-1.9151e-01, -8.0938e-01, -7.4878e-01,  ...,  8.7690e-02,\n           -6.5998e-02, -8.7660e-02],\n          [-1.4635e-02,  4.4938e-01, -1.3227e+00,  ..., -1.3625e+00,\n           -1.4243e+00, -8.3266e-01],\n          [ 1.1510e+00,  5.9788e-01, -1.3344e+00,  ..., -2.9452e-01,\n           -1.1426e+00,  4.3165e-04],\n          ...,\n          [ 3.5482e-01,  2.9180e-01, -4.7351e-01,  ..., -1.0602e+00,\n           -9.5300e-01, -1.4889e-01],\n          [ 1.1904e+00,  3.3002e-01,  7.0450e-01,  ...,  5.8851e-01,\n           -1.3167e+00,  1.1598e+00],\n          [ 3.0639e-01,  2.4975e-01,  2.0160e-01,  ..., -6.6445e-01,\n            7.7924e-01,  3.1040e-01]]]], grad_fn=<ConvolutionBackward0>)\n                                      Trace Shapes:                        \n                                       Param Sites:                        \n             mvae$$$image_encoder.features.0.weight  32   3       3       3\n             mvae$$$image_encoder.features.2.weight  64  32       3       3\n             mvae$$$image_encoder.features.4.weight                      64\n               mvae$$$image_encoder.features.4.bias                      64\n             mvae$$$image_encoder.features.6.weight 128  64       3       3\n             mvae$$$image_encoder.features.8.weight                     128\n               mvae$$$image_encoder.features.8.bias                     128\n             mvae$$$image_encoder.features.9.weight 256 128       3       3\n            mvae$$$image_encoder.features.11.weight                     256\n              mvae$$$image_encoder.features.11.bias                     256\n          mvae$$$image_encoder.z_loc_layer.0.weight             512  262144\n            mvae$$$image_encoder.z_loc_layer.0.bias                     512\n          mvae$$$image_encoder.z_loc_layer.3.weight              50     512\n            mvae$$$image_encoder.z_loc_layer.3.bias                      50\n        mvae$$$image_encoder.z_scale_layer.0.weight             512  262144\n          mvae$$$image_encoder.z_scale_layer.0.bias                     512\n        mvae$$$image_encoder.z_scale_layer.3.weight              50     512\n          mvae$$$image_encoder.z_scale_layer.3.bias                      50\n             mvae$$$image_decoder.upsample.0.weight         4194304      50\n               mvae$$$image_decoder.upsample.0.bias                 4194304\n          mvae$$$image_decoder.hallucinate.0.weight 256 128       3       3\n          mvae$$$image_decoder.hallucinate.1.weight                     128\n            mvae$$$image_decoder.hallucinate.1.bias                     128\n          mvae$$$image_decoder.hallucinate.3.weight 128  64       3       3\n          mvae$$$image_decoder.hallucinate.4.weight                      64\n            mvae$$$image_decoder.hallucinate.4.bias                      64\n          mvae$$$image_decoder.hallucinate.6.weight  64  32       3       3\n          mvae$$$image_decoder.hallucinate.7.weight                      32\n            mvae$$$image_decoder.hallucinate.7.bias                      32\n          mvae$$$image_decoder.hallucinate.9.weight  32   3       3       3\n                  mvae$$$emotion_encoder.net.weight             512       8\n                    mvae$$$emotion_encoder.net.bias                     512\n        mvae$$$emotion_encoder.z_loc_layer.0.weight             512     512\n          mvae$$$emotion_encoder.z_loc_layer.0.bias                     512\n        mvae$$$emotion_encoder.z_loc_layer.2.weight              50     512\n          mvae$$$emotion_encoder.z_loc_layer.2.bias                      50\n      mvae$$$emotion_encoder.z_scale_layer.0.weight             512     512\n        mvae$$$emotion_encoder.z_scale_layer.0.bias                     512\n      mvae$$$emotion_encoder.z_scale_layer.2.weight              50     512\n        mvae$$$emotion_encoder.z_scale_layer.2.bias                      50\n                mvae$$$emotion_decoder.net.0.weight             512      50\n                  mvae$$$emotion_decoder.net.0.bias                     512\n  mvae$$$emotion_decoder.emotion_loc_layer.0.weight             512     512\n    mvae$$$emotion_decoder.emotion_loc_layer.0.bias                     512\n  mvae$$$emotion_decoder.emotion_loc_layer.2.weight               8     512\n    mvae$$$emotion_decoder.emotion_loc_layer.2.bias                       8\nmvae$$$emotion_decoder.emotion_scale_layer.0.weight             512     512\n  mvae$$$emotion_decoder.emotion_scale_layer.0.bias                     512\nmvae$$$emotion_decoder.emotion_scale_layer.2.weight               8     512\n  mvae$$$emotion_decoder.emotion_scale_layer.2.bias                       8\n                                      Sample Sites:                        \n                                             z dist       8      50       |\n                                              value       8      50       |"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_elbo = []\n",
    "trainingTimes = [time.time()]\n",
    "# training loop\n",
    "for epoch in range(args.num_epochs):\n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    # do a training epoch over each mini-batch returned\n",
    "    # by the data loader\n",
    "    for batch_num, sample in tqdm(enumerate(trainset_loader)):\n",
    "        faces, emotions = sample['image'], sample['cat']\n",
    "        \n",
    "        emotions = torch.stack([emotion_rating_conversion(emo) for emo in emotions])\n",
    "        \n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if args.cuda:\n",
    "            faces, ratings, outcomes = faces.cuda(), ratings.cuda(), outcomes.cuda()\n",
    "        \n",
    "        # do ELBO gradient and accumulate loss\n",
    "        #print(\"Batch: \", batch_num, \"out of\", len(train_loader))\n",
    "        epoch_loss += svi.step(images=faces, emotions=emotions)\n",
    "        epoch_loss += svi.step(images=faces, emotions=None)\n",
    "        epoch_loss += svi.step(images=None, emotions=emotions)\n",
    "        epoch_loss += svi.step(images=None, emotions=None)\n",
    "\n",
    "    # report training diagnostics\n",
    "    normalizer_train = len(trainset_loader)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    train_elbo.append(total_epoch_loss_train)\n",
    "    \n",
    "    # report training diagnostics\n",
    "    trainingTimes.append(time.time())\n",
    "    epoch_time = trainingTimes[-1] - trainingTimes[-2]\n",
    "    print(\"[epoch %03d]  time: %.2f, average training loss: %.4f\" % (epoch, epoch_time, total_epoch_loss_train))\n",
    "    #if ((epoch+1) % 50 == 0):\n",
    "        #pyro.get_param_store().save('trained_models/checkpoints/tutorial_mvae_pretrained_' + str(epoch) + '.save')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4104b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
