{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f038ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "\n",
    "from RAVDESS_dataset_util import *\n",
    "from EmoClassCNN import *\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53be2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/home/studenti/ballerini/datasets/RAVDESS_frames'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16f25304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size:  720\n",
      "test set size:  6480\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = len(emocat)\n",
    "IMG_SIZE = 100\n",
    "BATCH_SIZE = 8\n",
    "DEFAULT_Z_DIM = 50\n",
    "\n",
    "face_dataset = FaceEmotionDataset(root_dir=folder_path,\n",
    "                                    transform=transforms.Compose([\n",
    "                                        Rescale(IMG_SIZE), \n",
    "                                        CenterCrop(IMG_SIZE), \n",
    "                                        ToTensor()\n",
    "                                    ]))        \n",
    "\n",
    "trainingset_len = len(face_dataset) // 100 * 10\n",
    "testset_len = len(face_dataset) - trainingset_len\n",
    "\n",
    "print('training set size: ', trainingset_len)\n",
    "print('test set size: ', testset_len)\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(face_dataset, \n",
    "                                                    [trainingset_len, testset_len], \n",
    "                                                    generator=torch.Generator().manual_seed(42)\n",
    "                                                   )\n",
    "\n",
    "trainset_loader = DataLoader(train_set, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, num_workers=4)\n",
    "\n",
    "testset_loader = DataLoader(test_set, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, num_workers=4)\n",
    "\n",
    "dataset_loader = (trainset_loader, testset_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2038f1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_rating_conversion(cat):\n",
    "    ratings = torch.zeros(NUM_CLASSES)\n",
    "    ratings[cat] = 1\n",
    "    return ratings\n",
    "    \n",
    "#torch.argmax(emotion_rating_conversion(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd681233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "class Swish(nn.Module):\n",
    "    \"\"\"https://arxiv.org/abs/1710.05941\"\"\"\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25911061",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductOfExperts(nn.Module):\n",
    "    \"\"\"\n",
    "    Return parameters for product of independent experts.\n",
    "    See https://arxiv.org/pdf/1410.7827.pdf for equations.\n",
    "\n",
    "    @param loc: M x D for M experts\n",
    "    @param scale: M x D for M experts\n",
    "    \"\"\"\n",
    "    def forward(self, loc, scale, eps=1e-8):\n",
    "        scale = scale + eps # numerical constant for stability\n",
    "        # precision of i-th Gaussian expert (T = 1/sigma^2)\n",
    "        T = 1. / scale\n",
    "        product_loc = torch.sum(loc * T, dim=0) / torch.sum(T, dim=0)\n",
    "        product_scale = 1. / torch.sum(T, dim=0)\n",
    "        return product_loc, product_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ab376f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    define the PyTorch module that parametrizes q(z|image).\n",
    "    This goes from images to the latent z\n",
    "    \n",
    "    This is the standard DCGAN architecture.\n",
    "\n",
    "    @param z_dim: integer\n",
    "                  size of the tensor representing the latent random variable z\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, \n",
    "        #                padding=0, dilation=1, groups=1, bias=True)\n",
    "        # H_out = floor( (H_in + 2*padding - dilation(kernel_size-1) -1) / stride    +1)\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 4, 2, 1, bias=False),\n",
    "            Swish(),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            Swish(),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            Swish(),\n",
    "            nn.Conv2d(128, 256, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            Swish())\n",
    "        \n",
    "        # Here, we define two layers, one to give z_loc and one to give z_scale\n",
    "        self.z_loc_layer = nn.Sequential(\n",
    "            nn.Linear(256 * 9 * 9, 512), # it's 256 * 9 * 9 if input is 100x100.\n",
    "            Swish(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, z_dim))\n",
    "        \n",
    "        self.z_scale_layer = nn.Sequential(\n",
    "            nn.Linear(256 * 9 * 9, 512), # it's 256 * 9 * 9 if input is 100x100.\n",
    "            Swish(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, z_dim))\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    def forward(self, image):\n",
    "        hidden = self.features(image)\n",
    "        print(hidden.shape)\n",
    "        image = image.view(-1, 256 * 9 * 9) # it's 256 * 9 * 9 if input is 100x100.\n",
    "        z_loc = self.z_loc_layer(hidden)\n",
    "        z_scale = torch.exp(self.z_scale_layer(hidden)) #add exp so it's always positive\n",
    "        return z_loc, z_scale\n",
    "    \n",
    "class ImageDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    define the PyTorch module that parametrizes p(image|z).\n",
    "    This goes from the latent z to the images\n",
    "    \n",
    "    This is the standard DCGAN architecture.\n",
    "\n",
    "    @param z_dim: integer\n",
    "                  size of the tensor representing the latent random variable z\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim):\n",
    "        super(ImageDecoder, self).__init__()\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256 * 9 * 9),  # it's 256 * 9 * 9 if input is 100x100.\n",
    "            Swish())\n",
    "        \n",
    "        self.hallucinate = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            Swish(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            Swish(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            Swish(),\n",
    "            nn.ConvTranspose2d(32, 3, 4, 2, 1, bias=False))\n",
    "\n",
    "    def forward(self, z):\n",
    "        # the input will be a vector of size |z_dim|\n",
    "        z = self.upsample(z)\n",
    "        z = z.view(-1, 256, 9, 9) # it's 256 * 9 * 9 if input is 100x100.\n",
    "        # but if 100x100, the output image size is 96x96\n",
    "        image = self.hallucinate(z) # this is the image\n",
    "        return image  # NOTE: no sigmoid here. See train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b04433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    define the PyTorch module that parametrizes q(z|emotion category).\n",
    "    This goes from ratings to the latent z\n",
    "\n",
    "    @param z_dim: integer\n",
    "                  size of the tensor representing the latent random variable z\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim):\n",
    "        super(EmotionEncoder, self).__init__()\n",
    "        self.net = nn.Linear(NUM_CLASSES, 512)\n",
    "        \n",
    "        self.z_loc_layer = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            Swish(),\n",
    "            nn.Linear(512, z_dim))\n",
    "        \n",
    "        self.z_scale_layer = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            Swish(),\n",
    "            nn.Linear(512, z_dim))\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    def forward(self, emocat):\n",
    "        hidden = self.net(emocat)\n",
    "        z_loc = self.z_loc_layer(hidden)\n",
    "        z_scale = torch.exp(self.z_scale_layer(hidden))\n",
    "        return z_loc, z_scale\n",
    "\n",
    "\n",
    "class EmotionDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    define the PyTorch module that parametrizes p(emotion category|z).\n",
    "    This goes from the latent z to the ratings\n",
    "\n",
    "    @param z_dim: integer\n",
    "                  size of the tensor representing the latent random variable z\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim):\n",
    "        super(EmotionDecoder, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(z_dim, 512),\n",
    "            Swish())\n",
    "        \n",
    "        self.emotion_loc_layer = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            Swish(),\n",
    "            nn.Linear(512, len(emocat)))\n",
    "        \n",
    "        self.emotion_scale_layer = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            Swish(),\n",
    "            nn.Linear(512, NUM_CLASSES))\n",
    "\n",
    "    def forward(self, z):\n",
    "        #batch_size = z.size(0)\n",
    "        hidden = self.net(z)\n",
    "        emotion_loc = self.emotion_loc_layer(hidden)\n",
    "        emotion_scale = torch.exp(self.emotion_scale_layer(hidden))\n",
    "        # rating is going to be a |emotions| * 9 levels\n",
    "        #rating = h.view(batch_size, EMOTION_VAR_DIM, 9)\n",
    "        return emotion_loc, emotion_scale  # NOTE: no softmax here. See train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92995328",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    This class encapsulates the parameters (neural networks), models & guides needed to train a\n",
    "    multimodal variational auto-encoder.\n",
    "    Modified from https://github.com/mhw32/multimodal-vae-public\n",
    "    Multimodal Variational Autoencoder.\n",
    "\n",
    "    @param z_dim: integer\n",
    "                  size of the tensor representing the latent random variable z\n",
    "                  \n",
    "    Currently all the neural network dimensions are hard-coded; \n",
    "    in a future version will make them be inputs into the constructor\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, use_cuda=True):\n",
    "        super(MVAE, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.image_encoder = ImageEncoder(z_dim)\n",
    "        self.image_decoder = ImageDecoder(z_dim)\n",
    "        self.emotion_encoder = EmotionEncoder(z_dim)\n",
    "        self.emotion_decoder =EmotionDecoder(z_dim)\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        # relative weights of losses in the different modalities\n",
    "        self.LAMBDA_IMAGES = 1.0\n",
    "        self.LAMBDA_RATINGS = 50.0\n",
    "        self.LAMBDA_OUTCOMES = 100.0\n",
    "        \n",
    "        # using GPUs for faster training of the networks\n",
    "        if self.use_cuda:\n",
    "            self.cuda()\n",
    "            \n",
    "    def model(self, images=None, emotions=None, annealing_beta=1.0):\n",
    "        # register this pytorch module and all of its sub-modules with pyro\n",
    "        pyro.module(\"mvae\", self)\n",
    "        \n",
    "        batch_size = 0\n",
    "        if images is not None:\n",
    "            batch_size = images.size(0)\n",
    "        elif emotions is not None:\n",
    "            batch_size = emotions.size(0)\n",
    "        \n",
    "        with pyro.plate(\"data\"):      \n",
    "            \n",
    "            # sample the latent z from the (constant) prior, z ~ Normal(0,I)\n",
    "            z_prior_mean  = torch.zeros(size=[BATCH_SIZE, self.z_dim])\n",
    "            z_prior_scale = torch.exp(torch.zeros(size=[BATCH_SIZE, self.z_dim]))                \n",
    "            \n",
    "            # sample from prior (value will be sampled by guide when computing the ELBO)\n",
    "            with poutine.scale(scale=annealing_beta):\n",
    "                z = pyro.sample(\"z\", dist.Normal(z_loc, z_scale))\n",
    "\n",
    "            # decode the latent code z (image decoder)\n",
    "            img_loc = self.image_decoder.forward(z)\n",
    "            \n",
    "            # score against actual images\n",
    "            if images is not None:\n",
    "                with poutine.scale(scale=self.LAMBDA_IMAGES):\n",
    "                    pyro.sample(\"obs_img\", dist.Bernoulli(img_loc), obs=images)\n",
    "            \n",
    "            # decode the latent code z (emotion decoder)\n",
    "            emotion_loc, emotion_scale = self.emotion_decoder.forward(z)\n",
    "            if categories is not None:\n",
    "                with poutine.scale(scale=self.LAMBDA_RATINGS):\n",
    "                    pyro.sample(\"obs_emotion\", \n",
    "                                dist.Normal(emotions_loc, emotions_scale), \n",
    "                                obs=emotion_rating_conversion(emotions))\n",
    "\n",
    "            # return the loc so we can visualize it later\n",
    "            return img_loc, emotion_loc\n",
    "        \n",
    "    def guide(self, images=None, emotions=None, annealing_beta=1.0):\n",
    "        # register this pytorch module and all of its sub-modules with pyro\n",
    "        pyro.module(\"mvae\", self)\n",
    "        \n",
    "        batch_size = 0\n",
    "        if images is not None:\n",
    "            batch_size = images.size(0)\n",
    "        elif emotions is not None:\n",
    "            batch_size = emotions.size(0)\n",
    "            \n",
    "        with pyro.plate(\"data\"):\n",
    "            # use the encoder to get the parameters used to define q(z|x)\n",
    "                        \n",
    "            # initialize the prior expert.\n",
    "            # we initalize an additional dimension, along which we concatenate all the \n",
    "            #   different experts.\n",
    "            # self.experts() then combines the information from these different modalities\n",
    "            #   by multiplying the gaussians together\n",
    "            \n",
    "            z_loc = torch.zeros(torch.Size((1, batch_size, self.z_dim))) + 0.5\n",
    "            z_scale = torch.ones(torch.Size((1, batch_size, self.z_dim))) * 0.1\n",
    "            if self.use_cuda:\n",
    "                z_loc, z_scale = z_loc.cuda(), z_scale.cuda()\n",
    "                \n",
    "            if images is not None:\n",
    "                image_z_loc, image_z_scale = self.image_encoder.forward(images)\n",
    "                z_loc = torch.cat((z_loc, image_z_loc.unsqueeze(0)), dim=0)\n",
    "                z_scale = torch.cat((z_scale, image_z_scale.unsqueeze(0)), dim=0)\n",
    "            \n",
    "            if emotions is not None:\n",
    "                emotion_z_loc, emotion_z_scale = self.emotion_encoder.forward(emotions)\n",
    "                z_loc = torch.cat((z_loc, rating_z_loc.unsqueeze(0)), dim=0)\n",
    "                z_scale = torch.cat((z_scale, rating_z_scale.unsqueeze(0)), dim=0)\n",
    "            \n",
    "            z_loc, z_scale = self.experts(z_loc, z_scale)\n",
    "            # sample the latent z\n",
    "            with poutine.scale(scale=annealing_beta):\n",
    "                pyro.sample(\"latent\", dist.Normal(z_loc, z_scale))\n",
    "                \n",
    "                \n",
    "    def forward(self, image=None, emotion=None):\n",
    "        z_loc, z_scale  = self.infer(image, emotion)\n",
    "        z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).independent(1))\n",
    "        # reconstruct inputs based on that gaussian\n",
    "        image_recon = self.image_decoder(z)\n",
    "        rating_recon = self.emotion_decoder(z)\n",
    "        return image_recon, rating_recon, z_loc, z_scale\n",
    "    \n",
    "    \n",
    "    def infer(self, images=None, emotions=None):\n",
    "        batch_size = 0\n",
    "        if images is not None:\n",
    "            batch_size = images.size(0)\n",
    "        elif emotions is not None:\n",
    "            batch_size = emotions.size(0)\n",
    "            \n",
    "        # initialize the prior expert\n",
    "        # we initalize an additional dimension, along which we concatenate all the \n",
    "        #   different experts.\n",
    "        # self.experts() then combines the information from these different modalities\n",
    "        #   by multiplying the gaussians together\n",
    "        z_loc = torch.zeros(torch.Size((1, BATCH_SIZE, self.z_dim))) + 0.5\n",
    "        z_scale = torch.ones(torch.Size((1, BATCH_SIXE, self.z_dim))) * 0.1\n",
    "        if self.use_cuda:\n",
    "            z_loc, z_scale = z_loc.cuda(), z_scale.cuda()\n",
    "\n",
    "        if images is not None:\n",
    "            image_z_loc, image_z_scale = self.image_encoder.forward(images)\n",
    "            z_loc = torch.cat((z_loc, image_z_loc.unsqueeze(0)), dim=0)\n",
    "            z_scale = torch.cat((z_scale, image_z_scale.unsqueeze(0)), dim=0)\n",
    "\n",
    "        if emotions is not None:\n",
    "            emotion_z_loc, emotion_z_scale = self.emotion_encoder.forward(emotions)\n",
    "            z_loc = torch.cat((z_loc, emotion_z_loc.unsqueeze(0)), dim=0)\n",
    "            z_scale = torch.cat((z_scale, emotion_z_scale.unsqueeze(0)), dim=0)\n",
    "\n",
    "        z_loc, z_scale = self.experts(z_loc, z_scale)\n",
    "        return z_loc, z_scale\n",
    "\n",
    "    \n",
    "    # define a helper function for reconstructing images\n",
    "    def reconstruct_img(self, images):\n",
    "        # encode image x\n",
    "        z_loc, z_scale = self.image_encoder(images)\n",
    "        # sample in latent space\n",
    "        z = dist.Normal(z_loc, z_scale).sample()\n",
    "        # decode the image (note we don't sample in image space)\n",
    "        img_loc = self.image_decoder.forward(z)\n",
    "        return img_loc\n",
    "\n",
    "    \n",
    "    # define a helper function for reconstructing images without sampling\n",
    "    def reconstruct_img_nosample(self, images):\n",
    "        # encode image x\n",
    "        z_loc, z_scale = self.image_encoder(images)\n",
    "        ## sample in latent space\n",
    "        #z = dist.Normal(z_loc, z_scale).sample()\n",
    "        # decode the image (note we don't sample in image space)\n",
    "        img_loc = self.image_decoder.forward(z_loc)\n",
    "        return img_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ff3a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "class Args:\n",
    "    learning_rate = 5e-6\n",
    "    num_epochs = 2 #500\n",
    "    z_dim = DEFAULT_Z_DIM\n",
    "    seed = 30\n",
    "    cuda = False\n",
    "    \n",
    "args = Args()\n",
    "\n",
    "# setup the VAE\n",
    "mvae = MVAE(z_dim=args.z_dim, use_cuda=args.cuda)\n",
    "\n",
    "# setup the optimizer\n",
    "adam_args = {\"lr\": args.learning_rate}\n",
    "optimizer = Adam(adam_args)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(mvae.model, mvae.guide, optimizer, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32de2c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:01, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256, 9, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 20736]' is invalid for input of size 240000\n                                      Trace Shapes:                    \n                                       Param Sites:                    \n             mvae$$$image_encoder.features.0.weight  32   3     4     4\n             mvae$$$image_encoder.features.2.weight  64  32     4     4\n             mvae$$$image_encoder.features.3.weight                  64\n               mvae$$$image_encoder.features.3.bias                  64\n             mvae$$$image_encoder.features.5.weight 128  64     4     4\n             mvae$$$image_encoder.features.6.weight                 128\n               mvae$$$image_encoder.features.6.bias                 128\n             mvae$$$image_encoder.features.8.weight 256 128     4     4\n             mvae$$$image_encoder.features.9.weight                 256\n               mvae$$$image_encoder.features.9.bias                 256\n          mvae$$$image_encoder.z_loc_layer.0.weight           512 20736\n            mvae$$$image_encoder.z_loc_layer.0.bias                 512\n          mvae$$$image_encoder.z_loc_layer.3.weight            50   512\n            mvae$$$image_encoder.z_loc_layer.3.bias                  50\n        mvae$$$image_encoder.z_scale_layer.0.weight           512 20736\n          mvae$$$image_encoder.z_scale_layer.0.bias                 512\n        mvae$$$image_encoder.z_scale_layer.3.weight            50   512\n          mvae$$$image_encoder.z_scale_layer.3.bias                  50\n             mvae$$$image_decoder.upsample.0.weight         20736    50\n               mvae$$$image_decoder.upsample.0.bias               20736\n          mvae$$$image_decoder.hallucinate.0.weight 256 128     4     4\n          mvae$$$image_decoder.hallucinate.1.weight                 128\n            mvae$$$image_decoder.hallucinate.1.bias                 128\n          mvae$$$image_decoder.hallucinate.3.weight 128  64     4     4\n          mvae$$$image_decoder.hallucinate.4.weight                  64\n            mvae$$$image_decoder.hallucinate.4.bias                  64\n          mvae$$$image_decoder.hallucinate.6.weight  64  32     4     4\n          mvae$$$image_decoder.hallucinate.7.weight                  32\n            mvae$$$image_decoder.hallucinate.7.bias                  32\n          mvae$$$image_decoder.hallucinate.9.weight  32   3     4     4\n                  mvae$$$emotion_encoder.net.weight           512     8\n                    mvae$$$emotion_encoder.net.bias                 512\n        mvae$$$emotion_encoder.z_loc_layer.0.weight           512   512\n          mvae$$$emotion_encoder.z_loc_layer.0.bias                 512\n        mvae$$$emotion_encoder.z_loc_layer.2.weight            50   512\n          mvae$$$emotion_encoder.z_loc_layer.2.bias                  50\n      mvae$$$emotion_encoder.z_scale_layer.0.weight           512   512\n        mvae$$$emotion_encoder.z_scale_layer.0.bias                 512\n      mvae$$$emotion_encoder.z_scale_layer.2.weight            50   512\n        mvae$$$emotion_encoder.z_scale_layer.2.bias                  50\n                mvae$$$emotion_decoder.net.0.weight           512    50\n                  mvae$$$emotion_decoder.net.0.bias                 512\n  mvae$$$emotion_decoder.emotion_loc_layer.0.weight           512   512\n    mvae$$$emotion_decoder.emotion_loc_layer.0.bias                 512\n  mvae$$$emotion_decoder.emotion_loc_layer.2.weight             8   512\n    mvae$$$emotion_decoder.emotion_loc_layer.2.bias                   8\nmvae$$$emotion_decoder.emotion_scale_layer.0.weight           512   512\n  mvae$$$emotion_decoder.emotion_scale_layer.0.bias                 512\nmvae$$$emotion_decoder.emotion_scale_layer.2.weight             8   512\n  mvae$$$emotion_decoder.emotion_scale_layer.2.bias                   8\n                                      Sample Sites:                    ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/poutine/trace_messenger.py:174\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mMVAE.guide\u001b[0;34m(self, images, emotions, annealing_beta)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     image_z_loc, image_z_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     z_loc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((z_loc, image_z_loc\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mImageEncoder.forward\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(hidden\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 45\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# it's 256 * 9 * 9 if input is 100x100.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m z_loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_loc_layer(hidden)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 20736]' is invalid for input of size 240000",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     faces, ratings, outcomes \u001b[38;5;241m=\u001b[39m faces\u001b[38;5;241m.\u001b[39mcuda(), ratings\u001b[38;5;241m.\u001b[39mcuda(), outcomes\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# do ELBO gradient and accumulate loss\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#print(\"Batch: \", batch_num, \"out of\", len(train_loader))\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43msvi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfaces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memotions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memotions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m svi\u001b[38;5;241m.\u001b[39mstep(images\u001b[38;5;241m=\u001b[39mfaces, emotions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m svi\u001b[38;5;241m.\u001b[39mstep(images\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, emotions\u001b[38;5;241m=\u001b[39memotions)\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/infer/svi.py:145\u001b[0m, in \u001b[0;36mSVI.step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# get loss and compute gradients\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m poutine\u001b[38;5;241m.\u001b[39mtrace(param_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m param_capture:\n\u001b[0;32m--> 145\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_and_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m    148\u001b[0m     site[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munconstrained() \u001b[38;5;28;01mfor\u001b[39;00m site \u001b[38;5;129;01min\u001b[39;00m param_capture\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    149\u001b[0m )\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# actually perform gradient steps\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# torch.optim objects gets instantiated for any params that haven't been seen yet\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/infer/trace_elbo.py:140\u001b[0m, in \u001b[0;36mTrace_ELBO.loss_and_grads\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# grab a trace from the generator\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_trace, guide_trace \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_traces(model, guide, args, kwargs):\n\u001b[1;32m    141\u001b[0m     loss_particle, surrogate_loss_particle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_differentiable_loss_particle(\n\u001b[1;32m    142\u001b[0m         model_trace, guide_trace\n\u001b[1;32m    143\u001b[0m     )\n\u001b[1;32m    144\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_particle \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_particles\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/infer/elbo.py:182\u001b[0m, in \u001b[0;36mELBO._get_traces\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_particles):\n\u001b[0;32m--> 182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/infer/trace_elbo.py:57\u001b[0m, in \u001b[0;36mTrace_ELBO._get_trace\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, guide, args, kwargs):\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m    Returns a single trace from the guide, and the model that is run\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    against it.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     model_trace, guide_trace \u001b[38;5;241m=\u001b[39m \u001b[43mget_importance_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_plate_nesting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_validation_enabled():\n\u001b[1;32m     61\u001b[0m         check_if_enumerated(guide_trace)\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/infer/enum.py:60\u001b[0m, in \u001b[0;36mget_importance_trace\u001b[0;34m(graph_type, max_plate_nesting, model, guide, args, kwargs, detach)\u001b[0m\n\u001b[1;32m     58\u001b[0m     model_trace, guide_trace \u001b[38;5;241m=\u001b[39m unwrapped_guide\u001b[38;5;241m.\u001b[39mget_traces()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     guide_trace \u001b[38;5;241m=\u001b[39m \u001b[43mpoutine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_type\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m detach:\n\u001b[1;32m     64\u001b[0m         guide_trace\u001b[38;5;241m.\u001b[39mdetach_()\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/poutine/trace_messenger.py:198\u001b[0m, in \u001b[0;36mTraceHandler.get_trace\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m    :returns: data structure\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m    :rtype: pyro.poutine.Trace\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m    Calls this poutine and returns its trace instead of the function's return value.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsngr\u001b[38;5;241m.\u001b[39mget_trace()\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/poutine/trace_messenger.py:180\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m         exc \u001b[38;5;241m=\u001b[39m exc_type(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(exc_value, shapes))\n\u001b[1;32m    179\u001b[0m         exc \u001b[38;5;241m=\u001b[39m exc\u001b[38;5;241m.\u001b[39mwith_traceback(traceback)\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsngr\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39madd_node(\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_RETURN\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_RETURN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, value\u001b[38;5;241m=\u001b[39mret\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/poutine/trace_messenger.py:174\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsngr\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39madd_node(\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_INPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_INPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    172\u001b[0m )\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    176\u001b[0m     exc_type, exc_value, traceback \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mMVAE.guide\u001b[0;34m(self, images, emotions, annealing_beta)\u001b[0m\n\u001b[1;32m     93\u001b[0m     z_loc, z_scale \u001b[38;5;241m=\u001b[39m z_loc\u001b[38;5;241m.\u001b[39mcuda(), z_scale\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     image_z_loc, image_z_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     z_loc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((z_loc, image_z_loc\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     98\u001b[0m     z_scale \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((z_scale, image_z_scale\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mImageEncoder.forward\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     43\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures(image)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(hidden\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 45\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# it's 256 * 9 * 9 if input is 100x100.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m z_loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_loc_layer(hidden)\n\u001b[1;32m     47\u001b[0m z_scale \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_scale_layer(hidden)) \u001b[38;5;66;03m#add exp so it's always positive\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 20736]' is invalid for input of size 240000\n                                      Trace Shapes:                    \n                                       Param Sites:                    \n             mvae$$$image_encoder.features.0.weight  32   3     4     4\n             mvae$$$image_encoder.features.2.weight  64  32     4     4\n             mvae$$$image_encoder.features.3.weight                  64\n               mvae$$$image_encoder.features.3.bias                  64\n             mvae$$$image_encoder.features.5.weight 128  64     4     4\n             mvae$$$image_encoder.features.6.weight                 128\n               mvae$$$image_encoder.features.6.bias                 128\n             mvae$$$image_encoder.features.8.weight 256 128     4     4\n             mvae$$$image_encoder.features.9.weight                 256\n               mvae$$$image_encoder.features.9.bias                 256\n          mvae$$$image_encoder.z_loc_layer.0.weight           512 20736\n            mvae$$$image_encoder.z_loc_layer.0.bias                 512\n          mvae$$$image_encoder.z_loc_layer.3.weight            50   512\n            mvae$$$image_encoder.z_loc_layer.3.bias                  50\n        mvae$$$image_encoder.z_scale_layer.0.weight           512 20736\n          mvae$$$image_encoder.z_scale_layer.0.bias                 512\n        mvae$$$image_encoder.z_scale_layer.3.weight            50   512\n          mvae$$$image_encoder.z_scale_layer.3.bias                  50\n             mvae$$$image_decoder.upsample.0.weight         20736    50\n               mvae$$$image_decoder.upsample.0.bias               20736\n          mvae$$$image_decoder.hallucinate.0.weight 256 128     4     4\n          mvae$$$image_decoder.hallucinate.1.weight                 128\n            mvae$$$image_decoder.hallucinate.1.bias                 128\n          mvae$$$image_decoder.hallucinate.3.weight 128  64     4     4\n          mvae$$$image_decoder.hallucinate.4.weight                  64\n            mvae$$$image_decoder.hallucinate.4.bias                  64\n          mvae$$$image_decoder.hallucinate.6.weight  64  32     4     4\n          mvae$$$image_decoder.hallucinate.7.weight                  32\n            mvae$$$image_decoder.hallucinate.7.bias                  32\n          mvae$$$image_decoder.hallucinate.9.weight  32   3     4     4\n                  mvae$$$emotion_encoder.net.weight           512     8\n                    mvae$$$emotion_encoder.net.bias                 512\n        mvae$$$emotion_encoder.z_loc_layer.0.weight           512   512\n          mvae$$$emotion_encoder.z_loc_layer.0.bias                 512\n        mvae$$$emotion_encoder.z_loc_layer.2.weight            50   512\n          mvae$$$emotion_encoder.z_loc_layer.2.bias                  50\n      mvae$$$emotion_encoder.z_scale_layer.0.weight           512   512\n        mvae$$$emotion_encoder.z_scale_layer.0.bias                 512\n      mvae$$$emotion_encoder.z_scale_layer.2.weight            50   512\n        mvae$$$emotion_encoder.z_scale_layer.2.bias                  50\n                mvae$$$emotion_decoder.net.0.weight           512    50\n                  mvae$$$emotion_decoder.net.0.bias                 512\n  mvae$$$emotion_decoder.emotion_loc_layer.0.weight           512   512\n    mvae$$$emotion_decoder.emotion_loc_layer.0.bias                 512\n  mvae$$$emotion_decoder.emotion_loc_layer.2.weight             8   512\n    mvae$$$emotion_decoder.emotion_loc_layer.2.bias                   8\nmvae$$$emotion_decoder.emotion_scale_layer.0.weight           512   512\n  mvae$$$emotion_decoder.emotion_scale_layer.0.bias                 512\nmvae$$$emotion_decoder.emotion_scale_layer.2.weight             8   512\n  mvae$$$emotion_decoder.emotion_scale_layer.2.bias                   8\n                                      Sample Sites:                    "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_elbo = []\n",
    "trainingTimes = [time.time()]\n",
    "# training loop\n",
    "for epoch in range(args.num_epochs):\n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    # do a training epoch over each mini-batch returned\n",
    "    # by the data loader\n",
    "    for batch_num, sample in tqdm(enumerate(trainset_loader)):\n",
    "        faces, emotions = sample['image'], sample['cat']\n",
    "        \n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if args.cuda:\n",
    "            faces, ratings, outcomes = faces.cuda(), ratings.cuda(), outcomes.cuda()\n",
    "        \n",
    "        # do ELBO gradient and accumulate loss\n",
    "        #print(\"Batch: \", batch_num, \"out of\", len(train_loader))\n",
    "        epoch_loss += svi.step(images=faces, emotions=emotions)\n",
    "        epoch_loss += svi.step(images=faces, emotions=None)\n",
    "        epoch_loss += svi.step(images=None, emotions=emotions)\n",
    "        epoch_loss += svi.step(images=None, emotions=None)\n",
    "\n",
    "    # report training diagnostics\n",
    "    normalizer_train = len(trainset_loader)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    train_elbo.append(total_epoch_loss_train)\n",
    "    \n",
    "    # report training diagnostics\n",
    "    trainingTimes.append(time.time())\n",
    "    epoch_time = trainingTimes[-1] - trainingTimes[-2]\n",
    "    print(\"[epoch %03d]  time: %.2f, average training loss: %.4f\" % (epoch, epoch_time, total_epoch_loss_train))\n",
    "    #if ((epoch+1) % 50 == 0):\n",
    "        #pyro.get_param_store().save('trained_models/checkpoints/tutorial_mvae_pretrained_' + str(epoch) + '.save')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7461556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
