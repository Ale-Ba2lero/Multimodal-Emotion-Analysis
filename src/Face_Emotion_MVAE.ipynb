{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad139ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "\n",
    "from RAVDESS_dataset_util import *\n",
    "from EmoClassCNN import *\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "pyro.enable_validation(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93157fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/home/studenti/ballerini/datasets/RAVDESS_frames'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b95168e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size:  6480\n",
      "test set size:  720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'testset_loader = DataLoader(test_set, batch_size=BATCH_SIZE,\\n                        shuffle=True, num_workers=4)\\n\\ndataset_loader = (trainset_loader, testset_loader)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = len(emocat)\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 8\n",
    "DEFAULT_Z_DIM = 50\n",
    "\n",
    "face_dataset = FaceEmotionDataset(root_dir=folder_path,\n",
    "                                    transform=transforms.Compose([\n",
    "                                        Rescale(IMG_SIZE), \n",
    "                                        CenterCrop(IMG_SIZE), \n",
    "                                        ToTensor()\n",
    "                                    ]))        \n",
    "\n",
    "trainingset_len = len(face_dataset) // 100 * 90\n",
    "testset_len = len(face_dataset) - trainingset_len\n",
    "\n",
    "print('training set size: ', trainingset_len)\n",
    "print('test set size: ', testset_len)\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(face_dataset, \n",
    "                                                    [trainingset_len, testset_len], \n",
    "                                                    generator=torch.Generator().manual_seed(42)\n",
    "                                                   )\n",
    "\n",
    "trainset_loader = DataLoader(train_set, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, num_workers=16)\n",
    "\n",
    "'''testset_loader = DataLoader(test_set, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, num_workers=4)\n",
    "\n",
    "dataset_loader = (trainset_loader, testset_loader)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e785e94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_rating_conversion(cat):\n",
    "    ratings = torch.zeros(NUM_CLASSES)\n",
    "    ratings[cat] = 1\n",
    "    return ratings\n",
    "    \n",
    "#torch.argmax(emotion_rating_conversion(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53b28f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "class Swish(nn.Module):\n",
    "    \"\"\"https://arxiv.org/abs/1710.05941\"\"\"\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e70d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductOfExperts(nn.Module):\n",
    "    \"\"\"\n",
    "    Return parameters for product of independent experts.\n",
    "    See https://arxiv.org/pdf/1410.7827.pdf for equations.\n",
    "\n",
    "    @param loc: M x D for M experts\n",
    "    @param scale: M x D for M experts\n",
    "    \"\"\"\n",
    "    def forward(self, loc, scale, eps=1e-8, use_cuda=True):\n",
    "        scale = scale + eps # numerical constant for stability\n",
    "        # precision of i-th Gaussian expert (T = 1/sigma^2)\n",
    "        T = 1. / scale\n",
    "        product_loc = torch.sum(loc * T, dim=0) / torch.sum(T, dim=0)\n",
    "        product_scale = 1. / torch.sum(T, dim=0)\n",
    "        return product_loc, product_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83b3c741",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    define the PyTorch module that parametrizes q(z|image).\n",
    "    This goes from images to the latent z\n",
    "    \n",
    "    This is the standard DCGAN architecture.\n",
    "\n",
    "    @param z_dim: integer\n",
    "                  size of the tensor representing the latent random variable z\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, img_size):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, \n",
    "        #                padding=0, dilation=1, groups=1, bias=True)\n",
    "        # H_out = floor( (H_in + 2*padding - dilation(kernel_size-1) -1) / stride    +1)\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, 1, 1, bias=False),\n",
    "            Swish(),\n",
    "            \n",
    "            nn.Conv2d(16, 32, 3, 1, 1, bias=False),\n",
    "            Swish(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3, 1, 1, bias=False),\n",
    "            Swish(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, 1, 1, bias=False),\n",
    "            Swish(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        \n",
    "        # Here, we define two layers, one to give z_loc and one to give z_scale\n",
    "        self.z_loc_layer = nn.Sequential(\n",
    "            nn.Linear(128 * (self.img_size // 4)**2, 256),\n",
    "            Swish(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(256, z_dim))\n",
    "        \n",
    "        self.z_scale_layer = nn.Sequential(\n",
    "            nn.Linear(128 * (self.img_size // 4)**2, 256),\n",
    "            Swish(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(256, z_dim))\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    def forward(self, image):\n",
    "        hidden = self.features(image)\n",
    "        hidden = hidden.view(-1, 128 * (self.img_size // 4)**2)\n",
    "        z_loc = self.z_loc_layer(hidden)\n",
    "        z_scale = torch.exp(self.z_scale_layer(hidden)) #add exp so it's always positive\n",
    "        return z_loc, z_scale\n",
    "    \n",
    "class ImageDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    define the PyTorch module that parametrizes p(image|z).\n",
    "    This goes from the latent z to the images\n",
    "    \n",
    "    This is the standard DCGAN architecture.\n",
    "\n",
    "    @param z_dim: integer\n",
    "                  size of the tensor representing the latent random variable z\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, img_size):\n",
    "        super(ImageDecoder, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Linear(z_dim, 128 * (self.img_size**2)),\n",
    "            Swish())\n",
    "        \n",
    "        self.hallucinate = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            Swish(),\n",
    "            nn.ConvTranspose2d(64, 32, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            Swish(),\n",
    "            nn.ConvTranspose2d(32, 16, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            Swish(),\n",
    "            nn.ConvTranspose2d(16, 3, 3, 1, 1, bias=False))\n",
    "\n",
    "    def forward(self, z):\n",
    "        # the input will be a vector of size |z_dim|\n",
    "        z = self.upsample(z)\n",
    "        z = z.view(-1, 128, self.img_size, self.img_size)\n",
    "        image = self.hallucinate(z) # this is the image\n",
    "        return image  # NOTE: no sigmoid here. See train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66c2eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    define the PyTorch module that parametrizes q(z|emotion category).\n",
    "    This goes from ratings to the latent z\n",
    "\n",
    "    @param z_dim: integer\n",
    "                  size of the tensor representing the latent random variable z\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, use_cuda=True):\n",
    "        super(EmotionEncoder, self).__init__()\n",
    "        self.net = nn.Linear(NUM_CLASSES, 265)\n",
    "        \n",
    "        self.z_loc_layer = nn.Sequential(\n",
    "            nn.Linear(265, 265),\n",
    "            Swish(),\n",
    "            nn.Linear(265, z_dim))\n",
    "        \n",
    "        self.z_scale_layer = nn.Sequential(\n",
    "            nn.Linear(265, 265),\n",
    "            Swish(),\n",
    "            nn.Linear(265, z_dim))\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    def forward(self, emocat):\n",
    "        hidden = self.net(emocat)\n",
    "        z_loc = self.z_loc_layer(hidden)\n",
    "        z_scale = torch.exp(self.z_scale_layer(hidden))\n",
    "        return z_loc, z_scale\n",
    "\n",
    "\n",
    "class EmotionDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    define the PyTorch module that parametrizes p(emotion category|z).\n",
    "    This goes from the latent z to the ratings\n",
    "\n",
    "    @param z_dim: integer\n",
    "                  size of the tensor representing the latent random variable z\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim):\n",
    "        super(EmotionDecoder, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(z_dim, 265),\n",
    "            Swish())\n",
    "        \n",
    "        self.emotion_loc_layer = nn.Sequential(\n",
    "            nn.Linear(265, 265),\n",
    "            Swish(),\n",
    "            nn.Linear(265, len(emocat)))\n",
    "        \n",
    "        self.emotion_scale_layer = nn.Sequential(\n",
    "            nn.Linear(265, 265),\n",
    "            Swish(),\n",
    "            nn.Linear(265, NUM_CLASSES))\n",
    "\n",
    "    def forward(self, z):\n",
    "        #batch_size = z.size(0)\n",
    "        hidden = self.net(z)\n",
    "        emotion_loc = self.emotion_loc_layer(hidden)\n",
    "        emotion_scale = torch.exp(self.emotion_scale_layer(hidden))\n",
    "        # rating is going to be a |emotions| * 9 levels\n",
    "        #rating = h.view(batch_size, EMOTION_VAR_DIM, 9)\n",
    "        return emotion_loc, emotion_scale  # NOTE: no softmax here. See train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dca7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    This class encapsulates the parameters (neural networks), models & guides needed to train a\n",
    "    multimodal variational auto-encoder.\n",
    "    Modified from https://github.com/mhw32/multimodal-vae-public\n",
    "    Multimodal Variational Autoencoder.\n",
    "\n",
    "    @param z_dim: integer\n",
    "                  size of the tensor representing the latent random variable z\n",
    "                  \n",
    "    Currently all the neural network dimensions are hard-coded; \n",
    "    in a future version will make them be inputs into the constructor\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, img_size=128, use_cuda=True):\n",
    "        super(MVAE, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.img_size = img_size\n",
    "        self.experts = ProductOfExperts()\n",
    "        self.image_encoder = ImageEncoder(z_dim, img_size)\n",
    "        self.image_decoder = ImageDecoder(z_dim, img_size)\n",
    "        self.emotion_encoder = EmotionEncoder(z_dim)\n",
    "        self.emotion_decoder = EmotionDecoder(z_dim)\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        # relative weights of losses in the different modalities\n",
    "        self.LAMBDA_IMAGES = 1.0\n",
    "        self.LAMBDA_RATINGS = 50.0\n",
    "        \n",
    "        # using GPUs for faster training of the networks\n",
    "        if self.use_cuda:\n",
    "            self.cuda()\n",
    "            \n",
    "    def model(self, images=None, emotions=None, annealing_beta=1.0):\n",
    "        # register this pytorch module and all of its sub-modules with pyro\n",
    "        pyro.module(\"mvae\", self)\n",
    "        \n",
    "        batch_size = 0\n",
    "        if images is not None:\n",
    "            batch_size = images.size(0)\n",
    "        elif emotions is not None:\n",
    "            batch_size = emotions.size(0)\n",
    "        \n",
    "        with pyro.plate(\"data\"):      \n",
    "            # sample the latent z from the (constant) prior, z ~ Normal(0,I)\n",
    "            z_prior_loc  = torch.zeros(size=[batch_size, self.z_dim])\n",
    "            z_prior_scale = torch.exp(torch.zeros(size=[batch_size, self.z_dim]))     \n",
    "            \n",
    "            if self.use_cuda:\n",
    "                z_prior_loc, z_prior_scale = z_prior_loc.cuda(), z_prior_scale.cuda()\n",
    "            \n",
    "            # sample from prior (value will be sampled by guide when computing the ELBO)\n",
    "            with poutine.scale(scale=annealing_beta):\n",
    "                z = pyro.sample(\"z\", dist.Normal(z_prior_loc, z_prior_scale))\n",
    "\n",
    "            # decode the latent code z (image decoder)\n",
    "            img_loc = self.image_decoder.forward(z)\n",
    "            \n",
    "            # score against actual images\n",
    "            if images is not None:\n",
    "                with poutine.scale(scale=self.LAMBDA_IMAGES):\n",
    "                    #img_loc = (img_loc - torch.min(img_loc)) / (torch.max(img_loc) - torch.min(img_loc))\n",
    "                    pyro.sample(\"obs_img\", dist.Bernoulli(img_loc), obs=images)\n",
    "            \n",
    "            # decode the latent code z (emotion decoder)\n",
    "            emotion_loc, emotion_scale = self.emotion_decoder.forward(z)\n",
    "            if emotions is not None:\n",
    "                with poutine.scale(scale=self.LAMBDA_RATINGS):\n",
    "                    pyro.sample(\"obs_emotion\", dist.Normal(emotion_loc, emotion_scale), obs=emotions)\n",
    "\n",
    "            # return the loc so we can visualize it later\n",
    "            return img_loc, emotion_loc\n",
    "        \n",
    "    def guide(self, images=None, emotions=None, annealing_beta=1.0):\n",
    "        # register this pytorch module and all of its sub-modules with pyro\n",
    "        pyro.module(\"mvae\", self)\n",
    "        \n",
    "        batch_size = 0\n",
    "        if images is not None:\n",
    "            batch_size = images.size(0)\n",
    "        elif emotions is not None:\n",
    "            batch_size = emotions.size(0)\n",
    "            \n",
    "        with pyro.plate(\"data\"):\n",
    "            # use the encoder to get the parameters used to define q(z|x)\n",
    "                        \n",
    "            # initialize the prior expert.\n",
    "            # we initalize an additional dimension, along which we concatenate all the \n",
    "            #   different experts.\n",
    "            # self.experts() then combines the information from these different modalities\n",
    "            #   by multiplying the gaussians together\n",
    "            \n",
    "            z_loc = torch.zeros(torch.Size((1, batch_size, self.z_dim))) + 0.5\n",
    "            z_scale = torch.ones(torch.Size((1, batch_size, self.z_dim))) * 0.1\n",
    "            \n",
    "            if self.use_cuda:\n",
    "                z_loc, z_scale = z_loc.cuda(), z_scale.cuda()\n",
    "                \n",
    "            if images is not None:\n",
    "                image_z_loc, image_z_scale = self.image_encoder.forward(images)\n",
    "                z_loc = torch.cat((z_loc, image_z_loc.unsqueeze(0)), dim=0)\n",
    "                z_scale = torch.cat((z_scale, image_z_scale.unsqueeze(0)), dim=0)\n",
    "            \n",
    "            if emotions is not None:\n",
    "                emotion_z_loc, emotion_z_scale = self.emotion_encoder.forward(emotions)\n",
    "                z_loc = torch.cat((z_loc, emotion_z_loc.unsqueeze(0)), dim=0)\n",
    "                z_scale = torch.cat((z_scale, emotion_z_scale.unsqueeze(0)), dim=0)\n",
    "            \n",
    "            z_loc, z_scale = self.experts(z_loc, z_scale)\n",
    "            # sample the latent z\n",
    "            with poutine.scale(scale=annealing_beta):\n",
    "                pyro.sample(\"z\", dist.Normal(z_loc, z_scale))\n",
    "                \n",
    "                \n",
    "    def forward(self, image=None, emotion=None):\n",
    "        z_loc, z_scale  = self.infer(image, emotion)\n",
    "        z = pyro.sample(\"z\", dist.Normal(z_loc, z_scale).independent(1))\n",
    "        # reconstruct inputs based on that gaussian\n",
    "        image_recon = self.image_decoder(z)\n",
    "        rating_recon = self.emotion_decoder(z)\n",
    "        return image_recon, rating_recon, z_loc, z_scale\n",
    "    \n",
    "    \n",
    "    def infer(self, images=None, emotions=None):\n",
    "        batch_size = 0\n",
    "        if images is not None:\n",
    "            batch_size = images.size(0)\n",
    "        elif emotions is not None:\n",
    "            batch_size = emotions.size(0)\n",
    "            \n",
    "        # initialize the prior expert\n",
    "        # we initalize an additional dimension, along which we concatenate all the \n",
    "        #   different experts.\n",
    "        # self.experts() then combines the information from these different modalities\n",
    "        #   by multiplying the gaussians together\n",
    "        z_loc = torch.zeros(torch.Size((1, BATCH_SIZE, self.z_dim)))+ 0.5\n",
    "        z_scale = torch.ones(torch.Size((1, BATCH_SIXE, self.z_dim)))* 0.1\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            z_loc, z_scale = z_loc.cuda(), z_scale.cuda()\n",
    "\n",
    "        if images is not None:\n",
    "            image_z_loc, image_z_scale = self.image_encoder.forward(images)\n",
    "            z_loc = torch.cat((z_loc, image_z_loc.unsqueeze(0)), dim=0)\n",
    "            z_scale = torch.cat((z_scale, image_z_scale.unsqueeze(0)), dim=0)\n",
    "\n",
    "        if emotions is not None:\n",
    "            emotion_z_loc, emotion_z_scale = self.emotion_encoder.forward(emotions)\n",
    "            z_loc = torch.cat((z_loc, emotion_z_loc.unsqueeze(0)), dim=0)\n",
    "            z_scale = torch.cat((z_scale, emotion_z_scale.unsqueeze(0)), dim=0)\n",
    "\n",
    "        z_loc, z_scale = self.experts(z_loc, z_scale)\n",
    "        return z_loc, z_scale\n",
    "\n",
    "    \n",
    "    # define a helper function for reconstructing images\n",
    "    def reconstruct_img(self, images):\n",
    "        # encode image x\n",
    "        z_loc, z_scale = self.image_encoder(images)\n",
    "        # sample in latent space\n",
    "        z = dist.Normal(z_loc, z_scale).sample()\n",
    "        # decode the image (note we don't sample in image space)\n",
    "        img_loc = self.image_decoder.forward(z)\n",
    "        return img_loc\n",
    "\n",
    "    \n",
    "    # define a helper function for reconstructing images without sampling\n",
    "    def reconstruct_img_nosample(self, images):\n",
    "        # encode image x\n",
    "        z_loc, z_scale = self.image_encoder(images)\n",
    "        ## sample in latent space\n",
    "        #z = dist.Normal(z_loc, z_scale).sample()\n",
    "        # decode the image (note we don't sample in image space)\n",
    "        img_loc = self.image_decoder.forward(z_loc)\n",
    "        return img_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43fc48dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "pyro.clear_param_store()\n",
    "\n",
    "class Args:\n",
    "    learning_rate = 5e-6\n",
    "    num_epochs = 10 #500\n",
    "    z_dim = DEFAULT_Z_DIM\n",
    "    img_size = IMG_SIZE\n",
    "    seed = 30\n",
    "    cuda = True\n",
    "    \n",
    "args = Args()\n",
    "\n",
    "# setup the optimizer\n",
    "adam_args = {\"lr\": args.learning_rate}\n",
    "optimizer = Adam(adam_args)\n",
    "\n",
    "# setup the VAE\n",
    "mvae = MVAE(z_dim=args.z_dim, img_size=args.img_size, use_cuda=args.cuda)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(mvae.model, mvae.guide, optimizer, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca984027",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|████████▎                                                                                      | 71/810 [00:22<03:57,  3.11it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m svi\u001b[38;5;241m.\u001b[39mstep(images\u001b[38;5;241m=\u001b[39mfaces, emotions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m svi\u001b[38;5;241m.\u001b[39mstep(images\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, emotions\u001b[38;5;241m=\u001b[39memotions)\n\u001b[0;32m---> 24\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43msvi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memotions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# report training diagnostics\u001b[39;00m\n\u001b[1;32m     27\u001b[0m normalizer_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(trainset_loader)\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/infer/svi.py:145\u001b[0m, in \u001b[0;36mSVI.step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# get loss and compute gradients\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m poutine\u001b[38;5;241m.\u001b[39mtrace(param_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m param_capture:\n\u001b[0;32m--> 145\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_and_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m    148\u001b[0m     site[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munconstrained() \u001b[38;5;28;01mfor\u001b[39;00m site \u001b[38;5;129;01min\u001b[39;00m param_capture\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    149\u001b[0m )\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# actually perform gradient steps\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# torch.optim objects gets instantiated for any params that haven't been seen yet\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/infer/trace_elbo.py:141\u001b[0m, in \u001b[0;36mTrace_ELBO.loss_and_grads\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# grab a trace from the generator\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_trace, guide_trace \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_traces(model, guide, args, kwargs):\n\u001b[0;32m--> 141\u001b[0m     loss_particle, surrogate_loss_particle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_differentiable_loss_particle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide_trace\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_particle \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_particles\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# collect parameters to train from model and guide\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/infer/trace_elbo.py:90\u001b[0m, in \u001b[0;36mTrace_ELBO._differentiable_loss_particle\u001b[0;34m(self, model_trace, guide_trace)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, site \u001b[38;5;129;01min\u001b[39;00m model_trace\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m site[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 90\u001b[0m         elbo_particle \u001b[38;5;241m=\u001b[39m elbo_particle \u001b[38;5;241m+\u001b[39m \u001b[43mtorch_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43msite\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog_prob_sum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m         surrogate_elbo_particle \u001b[38;5;241m=\u001b[39m surrogate_elbo_particle \u001b[38;5;241m+\u001b[39m site[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_prob_sum\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, site \u001b[38;5;129;01min\u001b[39;00m guide_trace\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/infer/util.py:46\u001b[0m, in \u001b[0;36mtorch_item\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtorch_item\u001b[39m(x):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    Like ``x.item()`` for a :class:`~torch.Tensor`, but also works with numbers.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, numbers\u001b[38;5;241m.\u001b[39mNumber) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "train_elbo = []\n",
    "# training loop\n",
    "for epoch in range(args.num_epochs):\n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    # do a training epoch over each mini-batch returned\n",
    "    # by the data loader\n",
    "    for batch_num, sample in enumerate(tqdm(trainset_loader)):\n",
    "                                  \n",
    "        faces, emotions = sample['image'], sample['cat']\n",
    "        emotions = torch.stack([emotion_rating_conversion(emo) for emo in emotions])\n",
    "        \n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if args.cuda:\n",
    "            faces, emotions = faces.cuda(), emotions.cuda()\n",
    "        \n",
    "        # do ELBO gradient and accumulate loss\n",
    "        #print(\"Batch: \", batch_num, \"out of\", len(train_loader))\n",
    "        epoch_loss += svi.step(images=faces, emotions=emotions)\n",
    "        epoch_loss += svi.step(images=faces, emotions=None)\n",
    "        epoch_loss += svi.step(images=None, emotions=emotions)\n",
    "        epoch_loss += svi.step(images=None, emotions=None)\n",
    "\n",
    "    # report training diagnostics\n",
    "    normalizer_train = len(trainset_loader)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    train_elbo.append(total_epoch_loss_train)\n",
    "    \n",
    "    # report training diagnostics\n",
    "    print(\"average training loss: %.4f\" % (total_epoch_loss_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07739ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(0, args.num_epochs)\n",
    "plt.plot(epochs, train_elbo, 'g', label='Training loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483f892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 10\n",
    "input_array = np.zeros(shape=(IMG_SIZE, 1, 3), dtype=\"uint8\")\n",
    "reconstructed_array = np.zeros(shape=(IMG_SIZE, 1, 3), dtype=\"uint8\")\n",
    "    \n",
    "# pick NUM_SAMPLES random test images from the first mini-batch and\n",
    "# visualize how well we're reconstructing them\n",
    "\n",
    "faces = next(iter(trainset_loader))['image']\n",
    "if args.cuda:\n",
    "    faces = faces.cuda()\n",
    "\n",
    "reco_indices = np.random.randint(0, faces.size(0), NUM_SAMPLES)\n",
    "for index in reco_indices:\n",
    "    input_img = faces[index, :]\n",
    "    # storing the input image\n",
    "    input_img_display = np.array(input_img.cpu()*255., dtype='uint8')\n",
    "    input_img_display = input_img_display.transpose((1, 2, 0))\n",
    "    input_array = np.concatenate((input_array, input_img_display), axis=1)\n",
    "\n",
    "    # generating the reconstructed image and adding to array\n",
    "    input_img = input_img.view(1, 3, IMG_SIZE, IMG_SIZE)\n",
    "    reconstructed_img = mvae.reconstruct_img_nosample(input_img)\n",
    "    reconstructed_img = reconstructed_img.cpu().view(3, IMG_SIZE, IMG_SIZE).detach().numpy()\n",
    "    reconstructed_img = np.array(reconstructed_img*255., dtype='uint8')\n",
    "    reconstructed_img = reconstructed_img.transpose((1, 2, 0))\n",
    "    reconstructed_array = np.concatenate((reconstructed_array, reconstructed_img), axis=1)\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# remove first, blank column, and concatenate\n",
    "input_array = input_array[:,1:,:]\n",
    "reconstructed_array = reconstructed_array[:,1:,:]\n",
    "display_array = np.concatenate((input_array, reconstructed_array), axis=0)\n",
    "Image.fromarray(display_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ce467e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
