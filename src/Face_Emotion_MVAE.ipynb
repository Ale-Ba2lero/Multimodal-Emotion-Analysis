{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad139ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "\n",
    "from RAVDESS_dataset_util import *\n",
    "from EmoClassCNN import *\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "pyro.enable_validation(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93157fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/home/studenti/ballerini/datasets/RAVDESS_frames'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b95168e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = len(emocat)\n",
    "IMG_SIZE = 100\n",
    "BATCH_SIZE = 8\n",
    "DEFAULT_Z_DIM = 50\n",
    "\n",
    "face_dataset = FaceEmotionDataset(root_dir=folder_path,\n",
    "                                    transform=transforms.Compose([\n",
    "                                        Rescale(IMG_SIZE), \n",
    "                                        CenterCrop(IMG_SIZE), \n",
    "                                        ToTensor()\n",
    "                                    ]))        \n",
    "\n",
    "dataset_loader = DataLoader(face_dataset, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, num_workers=20)\n",
    "\n",
    "print(len(face_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e785e94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_rating_conversion(cat):\n",
    "    ratings = torch.zeros(NUM_CLASSES)\n",
    "    ratings[cat] = 1\n",
    "    return ratings\n",
    "    \n",
    "#torch.argmax(emotion_rating_conversion(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53b28f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "class Swish(nn.Module):\n",
    "    \"\"\"https://arxiv.org/abs/1710.05941\"\"\"\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e70d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductOfExperts(nn.Module):\n",
    "    \"\"\"\n",
    "    Return parameters for product of independent experts.\n",
    "    See https://arxiv.org/pdf/1410.7827.pdf for equations.\n",
    "\n",
    "    @param loc: M x D for M experts\n",
    "    @param scale: M x D for M experts\n",
    "    \"\"\"\n",
    "    def forward(self, loc, scale, eps=1e-8, use_cuda=True):\n",
    "        scale = scale + eps # numerical constant for stability\n",
    "        # precision of i-th Gaussian expert (T = 1/sigma^2)\n",
    "        T = 1. / scale\n",
    "        product_loc = torch.sum(loc * T, dim=0) / torch.sum(T, dim=0)\n",
    "        product_scale = 1. / torch.sum(T, dim=0)\n",
    "        return product_loc, product_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83b3c741",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    define the PyTorch module that parametrizes q(z|image).\n",
    "    This goes from images to the latent z\n",
    "    \n",
    "    This is the standard DCGAN architecture.\n",
    "\n",
    "    @param z_dim: integer\n",
    "                  size of the tensor representing the latent random variable z\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, img_size):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, \n",
    "        #                padding=0, dilation=1, groups=1, bias=True)\n",
    "        # H_out = floor( (H_in + 2*padding - dilation(kernel_size-1) -1) / stride    +1)\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, 1, 1, bias=False), Swish(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Conv2d(16, 32, 3, 1, 1, bias=False), Swish(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 64, 3, 1, 1, bias=False), Swish(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 128, 3, 1, 1, bias=False), Swish(),\n",
    "            nn.BatchNorm2d(128),\n",
    "        )\n",
    "        \n",
    "        # Here, we define two layers, one to give z_loc and one to give z_scale\n",
    "        self.z_loc_layer = nn.Sequential(\n",
    "            nn.Linear(128 * (self.img_size**2), 128),\n",
    "            Swish(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(128, z_dim))\n",
    "        \n",
    "        self.z_scale_layer = nn.Sequential(\n",
    "            nn.Linear(128 * (self.img_size**2), 128),\n",
    "            Swish(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(128, z_dim))\n",
    "        \n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    def forward(self, image):\n",
    "        hidden = self.features(image)\n",
    "        hidden = hidden.view(-1, 128 * (self.img_size**2))\n",
    "        z_loc = self.z_loc_layer(hidden)\n",
    "        z_scale = torch.exp(self.z_scale_layer(hidden)) #add exp so it's always positive\n",
    "        return z_loc, z_scale\n",
    "    \n",
    "class ImageDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    define the PyTorch module that parametrizes p(image|z).\n",
    "    This goes from the latent z to the images\n",
    "    \n",
    "    This is the standard DCGAN architecture.\n",
    "\n",
    "    @param z_dim: integer\n",
    "                  size of the tensor representing the latent random variable z\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, img_size):\n",
    "        super(ImageDecoder, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Linear(z_dim, 128 * (self.img_size**2)),\n",
    "            Swish())\n",
    "        \n",
    "        self.hallucinate = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 3, 1, 1, bias=False), Swish(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ConvTranspose2d(64, 32, 3, 1, 1, bias=False), Swish(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ConvTranspose2d(32, 16, 3, 1, 1, bias=False), Swish(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ConvTranspose2d(16, 3, 3, 1, 1, bias=False))\n",
    "\n",
    "    def forward(self, z):\n",
    "        # the input will be a vector of size |z_dim|\n",
    "        z = self.upsample(z)\n",
    "        z = z.view(-1, 128, self.img_size, self.img_size)\n",
    "        image = self.hallucinate(z) # this is the image\n",
    "        return image  # NOTE: no sigmoid here. See train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66c2eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    define the PyTorch module that parametrizes q(z|emotion category).\n",
    "    This goes from ratings to the latent z\n",
    "\n",
    "    @param z_dim: integer\n",
    "                  size of the tensor representing the latent random variable z\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, use_cuda=True):\n",
    "        super(EmotionEncoder, self).__init__()\n",
    "        self.net = nn.Linear(NUM_CLASSES, 32)\n",
    "        \n",
    "        self.z_loc_layer = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            Swish(),\n",
    "            nn.Linear(32, z_dim))\n",
    "        \n",
    "        self.z_scale_layer = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            Swish(),\n",
    "            nn.Linear(32, z_dim))\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    def forward(self, emocat):\n",
    "        hidden = self.net(emocat)\n",
    "        z_loc = self.z_loc_layer(hidden)\n",
    "        z_scale = torch.exp(self.z_scale_layer(hidden))\n",
    "        return z_loc, z_scale\n",
    "\n",
    "\n",
    "class EmotionDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    define the PyTorch module that parametrizes p(emotion category|z).\n",
    "    This goes from the latent z to the ratings\n",
    "\n",
    "    @param z_dim: integer\n",
    "                  size of the tensor representing the latent random variable z\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim):\n",
    "        super(EmotionDecoder, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(z_dim, 32),\n",
    "            Swish())\n",
    "        \n",
    "        self.emotion_loc_layer = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            Swish(),\n",
    "            nn.Linear(32, NUM_CLASSES))\n",
    "        \n",
    "        self.emotion_scale_layer = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            Swish(),\n",
    "            nn.Linear(32, NUM_CLASSES))\n",
    "\n",
    "    def forward(self, z):\n",
    "        #batch_size = z.size(0)\n",
    "        hidden = self.net(z)\n",
    "        emotion_loc = self.emotion_loc_layer(hidden)\n",
    "        emotion_scale = torch.exp(self.emotion_scale_layer(hidden))\n",
    "        # rating is going to be a |emotions| * 9 levels\n",
    "        #rating = h.view(batch_size, EMOTION_VAR_DIM, 9)\n",
    "        return emotion_loc, emotion_scale  # NOTE: no softmax here. See train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dca7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    This class encapsulates the parameters (neural networks), models & guides needed to train a\n",
    "    multimodal variational auto-encoder.\n",
    "    Modified from https://github.com/mhw32/multimodal-vae-public\n",
    "    Multimodal Variational Autoencoder.\n",
    "\n",
    "    @param z_dim: integer\n",
    "                  size of the tensor representing the latent random variable z\n",
    "                  \n",
    "    Currently all the neural network dimensions are hard-coded; \n",
    "    in a future version will make them be inputs into the constructor\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, img_size=128, use_cuda=True):\n",
    "        super(MVAE, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.img_size = img_size\n",
    "        self.experts = ProductOfExperts()\n",
    "        self.image_encoder = ImageEncoder(z_dim, img_size)\n",
    "        self.image_decoder = ImageDecoder(z_dim, img_size)\n",
    "        self.emotion_encoder = EmotionEncoder(z_dim)\n",
    "        self.emotion_decoder = EmotionDecoder(z_dim)\n",
    "        \n",
    "        self.use_cuda = use_cuda\n",
    "        # relative weights of losses in the different modalities\n",
    "        self.LAMBDA_IMAGES = 1.0\n",
    "        self.LAMBDA_RATINGS = 50.0\n",
    "        \n",
    "        # using GPUs for faster training of the networks\n",
    "        if self.use_cuda:\n",
    "            self.cuda()\n",
    "            \n",
    "    def model(self, images=None, emotions=None, annealing_beta=1.0):\n",
    "        # register this pytorch module and all of its sub-modules with pyro\n",
    "        pyro.module(\"mvae\", self)\n",
    "        \n",
    "        batch_size = 0\n",
    "        if images is not None:\n",
    "            batch_size = images.size(0)\n",
    "        elif emotions is not None:\n",
    "            batch_size = emotions.size(0)\n",
    "        \n",
    "        with pyro.plate(\"data\"):      \n",
    "            # sample the latent z from the (constant) prior, z ~ Normal(0,I)\n",
    "            z_prior_loc  = torch.zeros(size=[batch_size, self.z_dim])\n",
    "            z_prior_scale = torch.exp(torch.zeros(size=[batch_size, self.z_dim]))     \n",
    "            \n",
    "            if self.use_cuda:\n",
    "                z_prior_loc, z_prior_scale = z_prior_loc.cuda(), z_prior_scale.cuda()\n",
    "            \n",
    "            # sample from prior (value will be sampled by guide when computing the ELBO)\n",
    "            with poutine.scale(scale=annealing_beta):\n",
    "                z = pyro.sample(\"z\", dist.Normal(z_prior_loc, z_prior_scale))\n",
    "\n",
    "            # decode the latent code z (image decoder)\n",
    "            img_loc = self.image_decoder.forward(z)\n",
    "            \n",
    "            # score against actual images\n",
    "            if images is not None:\n",
    "                with poutine.scale(scale=self.LAMBDA_IMAGES):\n",
    "                    #img_loc = (img_loc - torch.min(img_loc)) / (torch.max(img_loc) - torch.min(img_loc))\n",
    "                    pyro.sample(\"obs_img\", dist.Bernoulli(img_loc), obs=images)\n",
    "            \n",
    "            # decode the latent code z (emotion decoder)\n",
    "            emotion_loc, emotion_scale = self.emotion_decoder.forward(z)\n",
    "            if emotions is not None:\n",
    "                with poutine.scale(scale=self.LAMBDA_RATINGS):\n",
    "                    pyro.sample(\"obs_emotion\", dist.Normal(emotion_loc, emotion_scale), obs=emotions)\n",
    "\n",
    "            # return the loc so we can visualize it later\n",
    "            return img_loc, emotion_loc\n",
    "        \n",
    "    def guide(self, images=None, emotions=None, annealing_beta=1.0):\n",
    "        # register this pytorch module and all of its sub-modules with pyro\n",
    "        pyro.module(\"mvae\", self)\n",
    "        \n",
    "        batch_size = 0\n",
    "        if images is not None:\n",
    "            batch_size = images.size(0)\n",
    "        elif emotions is not None:\n",
    "            batch_size = emotions.size(0)\n",
    "            \n",
    "        with pyro.plate(\"data\"):\n",
    "            # use the encoder to get the parameters used to define q(z|x)\n",
    "                        \n",
    "            # initialize the prior expert.\n",
    "            # we initalize an additional dimension, along which we concatenate all the \n",
    "            #   different experts.\n",
    "            # self.experts() then combines the information from these different modalities\n",
    "            #   by multiplying the gaussians together\n",
    "            \n",
    "            z_loc = torch.zeros(torch.Size((1, batch_size, self.z_dim))) + 0.5\n",
    "            z_scale = torch.ones(torch.Size((1, batch_size, self.z_dim))) * 0.1\n",
    "            \n",
    "            if self.use_cuda:\n",
    "                z_loc, z_scale = z_loc.cuda(), z_scale.cuda()\n",
    "                \n",
    "            if images is not None:\n",
    "                image_z_loc, image_z_scale = self.image_encoder.forward(images)\n",
    "                z_loc = torch.cat((z_loc, image_z_loc.unsqueeze(0)), dim=0)\n",
    "                z_scale = torch.cat((z_scale, image_z_scale.unsqueeze(0)), dim=0)\n",
    "            \n",
    "            if emotions is not None:\n",
    "                emotion_z_loc, emotion_z_scale = self.emotion_encoder.forward(emotions)\n",
    "                z_loc = torch.cat((z_loc, emotion_z_loc.unsqueeze(0)), dim=0)\n",
    "                z_scale = torch.cat((z_scale, emotion_z_scale.unsqueeze(0)), dim=0)\n",
    "            \n",
    "            z_loc, z_scale = self.experts(z_loc, z_scale)\n",
    "            # sample the latent z\n",
    "            with poutine.scale(scale=annealing_beta):\n",
    "                pyro.sample(\"z\", dist.Normal(z_loc, z_scale))\n",
    "                \n",
    "                \n",
    "    def forward(self, image=None, emotion=None):\n",
    "        z_loc, z_scale  = self.infer(image, emotion)\n",
    "        z = pyro.sample(\"z\", dist.Normal(z_loc, z_scale).independent(1))\n",
    "        # reconstruct inputs based on that gaussian\n",
    "        image_recon = self.image_decoder(z)\n",
    "        rating_recon = self.emotion_decoder(z)\n",
    "        return image_recon, rating_recon, z_loc, z_scale\n",
    "    \n",
    "    \n",
    "    def infer(self, images=None, emotions=None):\n",
    "        batch_size = 0\n",
    "        if images is not None:\n",
    "            batch_size = images.size(0)\n",
    "        elif emotions is not None:\n",
    "            batch_size = emotions.size(0)\n",
    "            \n",
    "        # initialize the prior expert\n",
    "        # we initalize an additional dimension, along which we concatenate all the \n",
    "        #   different experts.\n",
    "        # self.experts() then combines the information from these different modalities\n",
    "        #   by multiplying the gaussians together\n",
    "        z_loc = torch.zeros(torch.Size((1, BATCH_SIZE, self.z_dim))) + 0.5\n",
    "        z_scale = torch.ones(torch.Size((1, BATCH_SIXE, self.z_dim))) * 0.1\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            z_loc, z_scale = z_loc.cuda(), z_scale.cuda()\n",
    "\n",
    "        if images is not None:\n",
    "            image_z_loc, image_z_scale = self.image_encoder.forward(images)\n",
    "            z_loc = torch.cat((z_loc, image_z_loc.unsqueeze(0)), dim=0)\n",
    "            z_scale = torch.cat((z_scale, image_z_scale.unsqueeze(0)), dim=0)\n",
    "\n",
    "        if emotions is not None:\n",
    "            emotion_z_loc, emotion_z_scale = self.emotion_encoder.forward(emotions)\n",
    "            z_loc = torch.cat((z_loc, emotion_z_loc.unsqueeze(0)), dim=0)\n",
    "            z_scale = torch.cat((z_scale, emotion_z_scale.unsqueeze(0)), dim=0)\n",
    "\n",
    "        z_loc, z_scale = self.experts(z_loc, z_scale)\n",
    "        return z_loc, z_scale\n",
    "\n",
    "    \n",
    "    # define a helper function for reconstructing images\n",
    "    def reconstruct_img(self, images):\n",
    "        # encode image x\n",
    "        z_loc, z_scale = self.image_encoder(images)\n",
    "        # sample in latent space\n",
    "        z = dist.Normal(z_loc, z_scale).sample()\n",
    "        # decode the image (note we don't sample in image space)\n",
    "        img_loc = self.image_decoder.forward(z)\n",
    "        return img_loc\n",
    "\n",
    "    \n",
    "    # define a helper function for reconstructing images without sampling\n",
    "    def reconstruct_img_nosample(self, images):\n",
    "        # encode image x\n",
    "        z_loc, z_scale = self.image_encoder(images)\n",
    "        ## sample in latent space\n",
    "        #z = dist.Normal(z_loc, z_scale).sample()\n",
    "        # decode the image (note we don't sample in image space)\n",
    "        img_loc = self.image_decoder.forward(z_loc)\n",
    "        return img_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43fc48dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "pyro.clear_param_store()\n",
    "\n",
    "class Args:\n",
    "    learning_rate = 1e-5\n",
    "    weight_decay = 1e-6\n",
    "    num_epochs = 50 #500\n",
    "    z_dim = DEFAULT_Z_DIM\n",
    "    img_size = IMG_SIZE\n",
    "    seed = 30\n",
    "    cuda = True\n",
    "    \n",
    "args = Args()\n",
    "\n",
    "# setup the optimizer\n",
    "adam_args = {\"lr\": args.learning_rate}#, \"weight_decay\":args.weight_decay}\n",
    "optimizer = Adam(adam_args)\n",
    "\n",
    "# setup the VAE\n",
    "mvae = MVAE(z_dim=args.z_dim, img_size=args.img_size, use_cuda=args.cuda)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(mvae.model, mvae.guide, optimizer, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca984027",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▌                         | 1/50 [07:53<6:26:34, 473.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average training loss: 7349660.6373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|█                         | 2/50 [16:36<6:41:57, 502.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average training loss: 7321895.4475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█                         | 2/50 [21:34<8:37:45, 647.21s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# do ELBO gradient and accumulate loss\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m#print(\"Batch: \", batch_num, \"out of\", len(train_loader))\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m svi\u001b[38;5;241m.\u001b[39mstep(images\u001b[38;5;241m=\u001b[39mfaces, emotions\u001b[38;5;241m=\u001b[39memotions)\n\u001b[0;32m---> 22\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43msvi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfaces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memotions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m svi\u001b[38;5;241m.\u001b[39mstep(images\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, emotions\u001b[38;5;241m=\u001b[39memotions)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# report training diagnostics\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/infer/svi.py:145\u001b[0m, in \u001b[0;36mSVI.step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# get loss and compute gradients\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m poutine\u001b[38;5;241m.\u001b[39mtrace(param_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m param_capture:\n\u001b[0;32m--> 145\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_and_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m    148\u001b[0m     site[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munconstrained() \u001b[38;5;28;01mfor\u001b[39;00m site \u001b[38;5;129;01min\u001b[39;00m param_capture\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    149\u001b[0m )\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# actually perform gradient steps\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# torch.optim objects gets instantiated for any params that haven't been seen yet\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/infer/trace_elbo.py:140\u001b[0m, in \u001b[0;36mTrace_ELBO.loss_and_grads\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# grab a trace from the generator\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_trace, guide_trace \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_traces(model, guide, args, kwargs):\n\u001b[1;32m    141\u001b[0m     loss_particle, surrogate_loss_particle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_differentiable_loss_particle(\n\u001b[1;32m    142\u001b[0m         model_trace, guide_trace\n\u001b[1;32m    143\u001b[0m     )\n\u001b[1;32m    144\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_particle \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_particles\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/infer/elbo.py:182\u001b[0m, in \u001b[0;36mELBO._get_traces\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_particles):\n\u001b[0;32m--> 182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/infer/trace_elbo.py:57\u001b[0m, in \u001b[0;36mTrace_ELBO._get_trace\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, guide, args, kwargs):\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m    Returns a single trace from the guide, and the model that is run\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    against it.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     model_trace, guide_trace \u001b[38;5;241m=\u001b[39m \u001b[43mget_importance_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_plate_nesting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_validation_enabled():\n\u001b[1;32m     61\u001b[0m         check_if_enumerated(guide_trace)\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/infer/enum.py:65\u001b[0m, in \u001b[0;36mget_importance_trace\u001b[0;34m(graph_type, max_plate_nesting, model, guide, args, kwargs, detach)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m detach:\n\u001b[1;32m     64\u001b[0m         guide_trace\u001b[38;5;241m.\u001b[39mdetach_()\n\u001b[0;32m---> 65\u001b[0m     model_trace \u001b[38;5;241m=\u001b[39m \u001b[43mpoutine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpoutine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguide_trace\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_type\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_validation_enabled():\n\u001b[1;32m     70\u001b[0m     check_model_guide_match(model_trace, guide_trace, max_plate_nesting)\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/poutine/trace_messenger.py:198\u001b[0m, in \u001b[0;36mTraceHandler.get_trace\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m    :returns: data structure\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m    :rtype: pyro.poutine.Trace\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m    Calls this poutine and returns its trace instead of the function's return value.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsngr\u001b[38;5;241m.\u001b[39mget_trace()\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/poutine/trace_messenger.py:174\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsngr\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39madd_node(\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_INPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_INPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    172\u001b[0m )\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    176\u001b[0m     exc_type, exc_value, traceback \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m~/miniconda3/envs/bll-env/lib/python3.9/site-packages/pyro/poutine/messenger.py:12\u001b[0m, in \u001b[0;36m_context_wrap\u001b[0;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_context_wrap\u001b[39m(context, fn, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m context:\n\u001b[0;32m---> 12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mMVAE.model\u001b[0;34m(self, images, emotions, annealing_beta)\u001b[0m\n\u001b[1;32m     46\u001b[0m z_prior_scale \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(torch\u001b[38;5;241m.\u001b[39mzeros(size\u001b[38;5;241m=\u001b[39m[batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_dim]))     \n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_cuda:\n\u001b[0;32m---> 49\u001b[0m     z_prior_loc, z_prior_scale \u001b[38;5;241m=\u001b[39m \u001b[43mz_prior_loc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, z_prior_scale\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# sample from prior (value will be sampled by guide when computing the ELBO)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m poutine\u001b[38;5;241m.\u001b[39mscale(scale\u001b[38;5;241m=\u001b[39mannealing_beta):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "train_elbo = []\n",
    "# training loop\n",
    "for epoch in tqdm(range(args.num_epochs)):\n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    # do a training epoch over each mini-batch returned\n",
    "    # by the data loader\n",
    "    for batch_num, sample in enumerate(dataset_loader):\n",
    "                                  \n",
    "        faces, emotions = sample['image'], sample['cat']\n",
    "        emotions = torch.stack([emotion_rating_conversion(emo) for emo in emotions])\n",
    "        \n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if args.cuda:\n",
    "            faces, emotions = faces.cuda(), emotions.cuda()\n",
    "        \n",
    "        # do ELBO gradient and accumulate loss\n",
    "        #print(\"Batch: \", batch_num, \"out of\", len(train_loader))\n",
    "        epoch_loss += svi.step(images=faces, emotions=emotions)\n",
    "        epoch_loss += svi.step(images=faces, emotions=None)\n",
    "        epoch_loss += svi.step(images=None, emotions=emotions)\n",
    "\n",
    "    # report training diagnostics\n",
    "    normalizer_train = len(dataset_loader)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    train_elbo.append(total_epoch_loss_train)\n",
    "    \n",
    "    # report training diagnostics\n",
    "    print(\"average training loss: %.4f\" % (total_epoch_loss_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede56965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(0, args.num_epochs)\n",
    "plt.plot(epochs, train_elbo, 'g', label='Training loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483f892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 10\n",
    "input_array = np.zeros(shape=(IMG_SIZE, 1, 3), dtype=\"uint8\")\n",
    "reconstructed_array = np.zeros(shape=(IMG_SIZE, 1, 3), dtype=\"uint8\")\n",
    "    \n",
    "# pick NUM_SAMPLES random test images from the first mini-batch and\n",
    "# visualize how well we're reconstructing them\n",
    "\n",
    "faces = next(iter(dataset_loader))['image']\n",
    "if args.cuda:\n",
    "    faces = faces.cuda()\n",
    "\n",
    "reco_indices = np.random.randint(0, faces.size(0), NUM_SAMPLES)\n",
    "for index in reco_indices:\n",
    "    input_img = faces[index, :]\n",
    "    # storing the input image\n",
    "    input_img_display = np.array(input_img.cpu()*255., dtype='uint8')\n",
    "    input_img_display = input_img_display.transpose((1, 2, 0))\n",
    "    input_array = np.concatenate((input_array, input_img_display), axis=1)\n",
    "\n",
    "    # generating the reconstructed image and adding to array\n",
    "    input_img = input_img.view(1, 3, IMG_SIZE, IMG_SIZE)\n",
    "    reconstructed_img = mvae.reconstruct_img_nosample(input_img)\n",
    "    reconstructed_img = reconstructed_img.cpu().view(3, IMG_SIZE, IMG_SIZE).detach().numpy()\n",
    "    reconstructed_img = np.array(reconstructed_img*255., dtype='uint8')\n",
    "    reconstructed_img = reconstructed_img.transpose((1, 2, 0))\n",
    "    reconstructed_array = np.concatenate((reconstructed_array, reconstructed_img), axis=1)\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# remove first, blank column, and concatenate\n",
    "input_array = input_array[:,1:,:]\n",
    "reconstructed_array = reconstructed_array[:,1:,:]\n",
    "display_array = np.concatenate((input_array, reconstructed_array), axis=0)\n",
    "Image.fromarray(display_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f4b7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# save model if you decide to modify the above code to train your own model\n",
    "savemodel = True\n",
    "if savemodel:\n",
    "    if not os.path.exists('./trained_models'):\n",
    "      os.mkdir('./trained_models')\n",
    "    pyro.get_param_store().save('trained_models/mvae_pretrained_02.save')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56daed33",
   "metadata": {},
   "source": [
    "min average training loss: 1,576,161.5907 model 01"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
