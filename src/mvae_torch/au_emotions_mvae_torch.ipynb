{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a8f36e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys; sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17f22e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "import multimodal_vae\n",
    "import torch_mvae_util as U\n",
    "from train_mvae import build_model, train\n",
    "from config_args import ConfigModelArgs, ConfigTrainArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38e7928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_model = ConfigModelArgs()\n",
    "cfg_train = ConfigTrainArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28cb7b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "au_dataset = pd.read_csv(cfg_model.dataset_path).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6282284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size:  7920 \n",
      "test set size:  1982\n"
     ]
    }
   ],
   "source": [
    "trainingset_len = len(au_dataset) // 100 * 80\n",
    "testset_len = len(au_dataset) - trainingset_len\n",
    "training_dataset, testing_dataset = torch.utils.data.random_split(\n",
    "    au_dataset, \n",
    "    [trainingset_len, testset_len],\n",
    "    generator=torch.Generator().manual_seed(66)\n",
    ")\n",
    "\n",
    "dataset_loader = DataLoader(training_dataset, batch_size=cfg_train.batch_size,\n",
    "                        shuffle=False, num_workers=cfg_train.num_workers)\n",
    "\n",
    "testset_loader = DataLoader(testing_dataset, batch_size=cfg_train.batch_size,\n",
    "                        shuffle=False, num_workers=cfg_train.num_workers)\n",
    "\n",
    "print('training set size: ',trainingset_len,'\\ntest set size: ',testset_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7c955ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat_dim': 8, 'au_dim': 18, 'latent_space_dim': 50, 'hidden_dim': 256, 'num_filters': 32, 'modes': {'au': <config_args.Mode object at 0x7fbdab543850>, 'face': None, 'emotion': <config_args.Mode object at 0x7fbda7c965f0>}, 'expert_type': 'poe', 'use_cuda': True}\n",
      "{'learning_rate': 5e-06, 'optim_betas': [0.95, 0.98], 'num_epochs': 10, 'batch_size': 32}\n"
     ]
    }
   ],
   "source": [
    "model_args = {\n",
    "    'cat_dim' : cfg_model.cat_dim,\n",
    "    'au_dim' : cfg_model.au_dim,\n",
    "    'latent_space_dim' : cfg_model.z_dim,\n",
    "    'hidden_dim' : cfg_model.hidden_dim,\n",
    "    'num_filters' : cfg_model.num_filters,\n",
    "    'modes' : cfg_model.modes,\n",
    "    'expert_type' : cfg_model.expert_type,\n",
    "    'use_cuda' : True\n",
    "}\n",
    "print(model_args)\n",
    "train_args = {\n",
    "    'learning_rate' : cfg_train.learning_rate,\n",
    "    'optim_betas' : cfg_train.optim_betas,\n",
    "    'num_epochs' : cfg_train.num_epochs,\n",
    "    'batch_size' : cfg_train.batch_size\n",
    "}\n",
    "print(train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acb4c5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model: torch.nn.Module = build_model(**model_args).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0e27a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False\n",
    "\n",
    "if load_model:    \n",
    "    PATH = \"../trained_models/fusion.save\"\n",
    "    loaded_data = torch.load(PATH)\n",
    "    \n",
    "    model: torch.nn.Module = build_model(**loaded_data['model_args']).double()\n",
    "    model.load_state_dict(loaded_data['model_params'])\n",
    "    \n",
    "    training_losses = loaded_data['training_loss']    \n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "133b91a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                              | 0/10 [00:00<?, ?it/s]/opt/conda/conda-bld/pytorch_1656352645774/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [0,0,0], thread: [15,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352645774/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [0,0,0], thread: [18,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352645774/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [0,0,0], thread: [20,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352645774/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [0,0,0], thread: [23,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352645774/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [0,0,0], thread: [30,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "  0%|                                                                              | 0/10 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasDgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m train_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_model:\n\u001b[0;32m----> 4\u001b[0m     training_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmvae_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptim_betas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim_betas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint_every\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Multimodal_RSA/src/mvae_torch/train_mvae.py:240\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(mvae_model, dataset_loader, learning_rate, optim_betas, num_epochs, batch_size, seed, use_cuda, cfg, checkpoint_every, resume_train)\u001b[0m\n\u001b[1;32m    238\u001b[0m         emotions \u001b[38;5;241m=\u001b[39m emotions\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# multimodal loss\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m m_losses: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43meval_model_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmvae_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannealing_beta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43memotions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memotions\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m multimodal_loss\u001b[38;5;241m.\u001b[39mtotal_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(m_losses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()))\n\u001b[1;32m    249\u001b[0m multimodal_loss\u001b[38;5;241m.\u001b[39mreconstruction_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(m_losses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreconstruction_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()))\n",
      "File \u001b[0;32m~/Multimodal_RSA/src/mvae_torch/train_mvae.py:153\u001b[0m, in \u001b[0;36meval_model_training\u001b[0;34m(model, optimizer, beta, au, faces, emotions)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_model_training\u001b[39m(\n\u001b[1;32m    136\u001b[0m         model,\n\u001b[1;32m    137\u001b[0m         optimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# Zero the parameter gradients\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    146\u001b[0m     (\n\u001b[1;32m    147\u001b[0m         au_reconstruction,\n\u001b[1;32m    148\u001b[0m         face_reconstruction,\n\u001b[1;32m    149\u001b[0m         emotion_reconstruction,\n\u001b[1;32m    150\u001b[0m         z_loc_expert,\n\u001b[1;32m    151\u001b[0m         z_scale_expert,\n\u001b[1;32m    152\u001b[0m         latent_sample\n\u001b[0;32m--> 153\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfaces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43memotions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memotions\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss_function(\n\u001b[1;32m    160\u001b[0m         au\u001b[38;5;241m=\u001b[39mau,\n\u001b[1;32m    161\u001b[0m         faces\u001b[38;5;241m=\u001b[39mfaces,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m         latent_sample\u001b[38;5;241m=\u001b[39mlatent_sample\n\u001b[1;32m    170\u001b[0m     )\n\u001b[1;32m    172\u001b[0m     loss[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/rsa-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Multimodal_RSA/src/mvae_torch/multimodal_vae.py:257\u001b[0m, in \u001b[0;36mMultimodalVariationalAutoencoder.forward\u001b[0;34m(self, au, faces, emotions)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m    252\u001b[0m             au: torch\u001b[38;5;241m.\u001b[39mtensor\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[1;32m    253\u001b[0m             faces: torch\u001b[38;5;241m.\u001b[39mtensor\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[1;32m    254\u001b[0m             emotions: torch\u001b[38;5;241m.\u001b[39mtensor\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    255\u001b[0m            ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;66;03m# Infer the latent distribution parameters\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m     z_loc, z_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_latent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43memotions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memotions\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# Sample from the latent space         \u001b[39;00m\n\u001b[1;32m    264\u001b[0m     epsilon: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(z_loc)\n",
      "File \u001b[0;32m~/Multimodal_RSA/src/mvae_torch/multimodal_vae.py:174\u001b[0m, in \u001b[0;36mMultimodalVariationalAutoencoder.infer_latent\u001b[0;34m(self, au, faces, emotions)\u001b[0m\n\u001b[1;32m    167\u001b[0m batch_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_batch_size_from_data(\n\u001b[1;32m    168\u001b[0m     au\u001b[38;5;241m=\u001b[39mau,\n\u001b[1;32m    169\u001b[0m     faces\u001b[38;5;241m=\u001b[39mfaces,\n\u001b[1;32m    170\u001b[0m     emotions\u001b[38;5;241m=\u001b[39memotions\n\u001b[1;32m    171\u001b[0m )\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_expert:\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_expert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfaces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memotions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_fusion(au, faces, emotions, batch_size)\n",
      "File \u001b[0;32m~/Multimodal_RSA/src/mvae_torch/multimodal_vae.py:235\u001b[0m, in \u001b[0;36mMultimodalVariationalAutoencoder.apply_expert\u001b[0;34m(self, au, faces, emotions, batch_size)\u001b[0m\n\u001b[1;32m    232\u001b[0m     z_scale \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((z_scale, face_z_scale\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m emotions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 235\u001b[0m     emotion_z_loc, emotion_z_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_emotion_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43memotions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m     z_loc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((z_loc, emotion_z_loc\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    237\u001b[0m     z_scale \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((z_scale, emotion_z_scale\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Multimodal_RSA/src/mvae_torch/nn_modules.py:286\u001b[0m, in \u001b[0;36mEmotionEncoder.forward\u001b[0;34m(self, emotion)\u001b[0m\n\u001b[1;32m    284\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet(emotion\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat64))\n\u001b[1;32m    285\u001b[0m z_loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_loc_layer(hidden)\n\u001b[0;32m--> 286\u001b[0m z_scale \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mz_scale_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z_loc, z_scale\n",
      "File \u001b[0;32m~/miniconda3/envs/rsa-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/rsa-env/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/rsa-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/rsa-env/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasDgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
     ]
    }
   ],
   "source": [
    "train_model = True\n",
    "\n",
    "if train_model:\n",
    "    training_losses = train(\n",
    "        mvae_model=model,\n",
    "        dataset_loader=dataset_loader,\n",
    "        learning_rate=cfg_train.learning_rate,\n",
    "        optim_betas=cfg_train.optim_betas,\n",
    "        num_epochs=cfg_train.num_epochs,\n",
    "        batch_size=cfg_train.batch_size,\n",
    "        seed=cfg_train.seed,\n",
    "        use_cuda=cfg_train.use_cuda,\n",
    "        cfg=cfg_train,\n",
    "        checkpoint_every=cfg_train.checkpoint_every,\n",
    "        resume_train = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89fe8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "U.print_losses(training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea99e109",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = U.images_to_images(model, testset_loader, num_images=8, model_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673a4409",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_image = U.emotions_to_images(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e3886c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_model = True\n",
    "\n",
    "if save_model:\n",
    "    PATH = \"../trained_models/au_emo.save\"\n",
    "    torch.save({#'rec_image' : rec_image,\n",
    "                'model_args' : model_args,\n",
    "                'training_loss' : training_losses,\n",
    "                'train_args': train_args,\n",
    "                'model_params' : model.state_dict()\n",
    "               }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f5941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "labels = list(Rd.emocat.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad716a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = U.classiffication_accuracy(model, testset_loader)\n",
    "cr = classification_report(y_true, y_pred, target_names=labels)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm = pd.DataFrame(cm, index = [i for i in labels],columns = [i for i in labels])\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, annot=True, fmt=\"d\")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd680971",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = U.recon_and_classiffication_accuracy(model, num_samples=2048)\n",
    "cr = classification_report(y_true, y_pred, target_names=labels)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm = pd.DataFrame(cm, index = [i for i in labels],columns = [i for i in labels])\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, annot=True, fmt=\"d\")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e83d51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rsa-env]",
   "language": "python",
   "name": "conda-env-rsa-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
