{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4562f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys; sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f11e6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import util.RAVDESS_dataset_util as Rd\n",
    "import multimodal_vae\n",
    "import train_mvae_plain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1555d653",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config_model_args:\n",
    "    img_size= 64\n",
    "    z_dim= 256\n",
    "    hidden_dim= 512\n",
    "    loss_weights = {'face': 1.0,'emocat': 1.0}\n",
    "    expert_type= \"moe\"\n",
    "    dataset_path= '/home/studenti/ballerini/datasets/RAVDESS_frames_ds'\n",
    "\n",
    "\n",
    "class Config_train_args:\n",
    "    learning_rate= 0.0001\n",
    "    optim_betas= [ 0.95, 0.98 ]\n",
    "    num_epochs= 100\n",
    "    batch_size= 32,\n",
    "    checkpoint_every= 20\n",
    "    checkpoint_path= \"./\"\n",
    "    save_model= True\n",
    "    model_save_path= \"trained_models/ravdess_mmvae_pretrained_plain.pt\",\n",
    "    stats_save_path= \"trained_models/ravdess_mmvae_pretrained_plain_stats.pt\",\n",
    "    seed= 100\n",
    "    use_cuda= True\n",
    "    annealing_type= \"static\"\n",
    "    cyclical_annealing= {\n",
    "      'min_beta': 0.0001,\n",
    "      'max_beta': 0.8,\n",
    "      'num_cycles': 8,\n",
    "      'annealing_percentage': 0.9\n",
    "    }\n",
    "    linear_annealing= {\n",
    "      'min_beta': 0.001,\n",
    "      'max_beta': 1.0,\n",
    "    }\n",
    "    static_annealing_beta= 0.1\n",
    "\n",
    "\n",
    "cfg_model = Config_model_args()\n",
    "cfg_train = Config_train_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d0a4018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_dataset = Rd.FaceEmotionDataset(\n",
    "    root_dir=cfg_model.dataset_path,\n",
    "    transform=transforms.Compose\n",
    "    ([\n",
    "        Rd.Rescale(cfg_model.img_size), \n",
    "        Rd.CenterCrop(cfg_model.img_size), \n",
    "        Rd.ToTensor()\n",
    "    ]))\n",
    "\n",
    "trainingset_len = len(face_dataset) // 100 * 90\n",
    "testset_len = len(face_dataset) - trainingset_len\n",
    "training_dataset, testing_dataset = torch.utils.data.random_split(face_dataset, \n",
    "                                                    [trainingset_len, testset_len], \n",
    "                                                    #generator=torch.Generator().manual_seed(42)\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "39a92a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = np.array(next(iter(face_dataset))['image']).transpose((1, 2, 0))\n",
    "# print(img.shape)\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f57e4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset, testing_dataset = util.load_preprocessed_dataset(cfg=cfg)\n",
    "\n",
    "input_dims: dict = util.get_modality_input_dimensions_from_data(\n",
    "    dataset=training_dataset,\n",
    "    modality=\"plain\"\n",
    ")\n",
    "    \n",
    "model: torch.nn.Module = build_model(\n",
    "    input_dims=input_dims,\n",
    "    latent_space_dim=cfg.model.plain.latent_space_dim,\n",
    "    va_mlp_hidden_dim=cfg.model.plain.va_mlp_hidden_dim,\n",
    "    loss_weights=cfg.model.plain.loss_weights,\n",
    "    expert_type=cfg.model.plain.expert_type,\n",
    "    use_cuda=cfg.train.plain.use_cuda\n",
    ")\n",
    "    \n",
    "train(\n",
    "    mmvae_model=model,\n",
    "    dataset=training_dataset,\n",
    "    learning_rate=cfg.train.plain.learning_rate,\n",
    "    optim_betas=cfg.train.plain.optim_betas,\n",
    "    num_epochs=cfg.train.plain.num_epochs,\n",
    "    batch_size=cfg.train.plain.batch_size,\n",
    "    checkpoint_every=cfg.train.plain.checkpoint_every,\n",
    "    checkpoint_path=cfg.train.plain.checkpoint_path,\n",
    "    save_model=cfg.train.plain.save_model,\n",
    "    seed=cfg.train.plain.seed,\n",
    "    use_cuda=cfg.train.plain.use_cuda,\n",
    "    cfg=cfg\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
